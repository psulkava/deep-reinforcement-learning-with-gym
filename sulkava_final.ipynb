{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSCI-E89 Deep Learning  \n",
    "Spring 2019  \n",
    "Final Project  \n",
    "Sulkava, Pietari  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers import Multiply\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "from atari_wrappers import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "episodes = 5000\n",
    "memory = deque(maxlen=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=observation_space, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(action_space, activation='linear'))\n",
    "model.compile(loss='mse', optimizer=Adam(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(batch_size):\n",
    "    # Sample minibatch from the memory\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "\n",
    "    # Extract informations from each memory\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "\n",
    "        # if done, make our target reward\n",
    "        target = reward\n",
    "\n",
    "        if not done:\n",
    "          # predict the future discounted reward\n",
    "          target = reward + gamma * \\\n",
    "                   np.amax(model.predict(next_state)[0])\n",
    "\n",
    "        # make the agent to approximately map\n",
    "        # the current state to future discounted reward\n",
    "        # We'll call that target_f\n",
    "        target_f = model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "\n",
    "        # Train the Neural Net with the state and target_f\n",
    "        model.fit(state, target_f, epochs=1, verbose=0)\n",
    "    \n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(state):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        # The agent acts randomly\n",
    "        return env.action_space.sample()\n",
    "    # Predict the reward value based on the given state\n",
    "    act_values = model.predict(state)\n",
    "    # Pick the action based on the predicted reward\n",
    "    return np.argmax(act_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/5000, score: 11\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-eae96fc4d495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-1c57f4bc0096>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Sample minibatch from the memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Extract informations from each memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, 4])\n",
    "    \n",
    "    for time_t in range(500):\n",
    "        env.render()\n",
    "        \n",
    "        action = act(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, 4])\n",
    "        \n",
    "        remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}\".format(e, episodes, time_t))\n",
    "            \n",
    "            break\n",
    "            \n",
    "    replay(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=1000000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.training_frequency = 4\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, total_step, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        if total_step % self.training_frequency == 0:\n",
    "            minibatch = random.sample(self.memory, batch_size)\n",
    "            for state, action, reward, next_state, done in minibatch:\n",
    "                target = reward\n",
    "                if not done:\n",
    "                    target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "                target_f = self.model.predict(state)\n",
    "                target_f[0][action] = target\n",
    "                self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            self.epsilon = max(self.epsilon, self.epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  4\n",
      "episode: 0/5000, score: 19\n",
      "episode: 1/5000, score: 20\n",
      "episode: 2/5000, score: 11\n",
      "episode: 3/5000, score: 40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-facdee007563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# train the agent with the experience of the episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-eb664d5a2b22>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize gym environment and the agent\n",
    "env = gym.make('CartPole-v1')\n",
    "observation_space = env.observation_space.shape[0]\n",
    "print('Observation space: ', observation_space)\n",
    "action_space = env.action_space.n\n",
    "agent = DQNAgent(observation_space, action_space)\n",
    "episodes = 5000\n",
    "# Iterate the game\n",
    "for e in range(episodes):\n",
    "    # reset state in the beginning of each game\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, observation_space])\n",
    "    # time_t represents each frame of the game\n",
    "    # Our goal is to keep the pole upright as long as possible until score of 500\n",
    "    # the more time_t the more score\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        # turn this on if you want to render\n",
    "        # env.render()\n",
    "        # Decide action\n",
    "        action = agent.act(state)\n",
    "        # Advance the game to the next frame based on the action.\n",
    "        # Reward is 1 for every frame the pole survived\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, observation_space])\n",
    "        # Remember the previous state, action, reward, and done\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "        # done becomes True when the game ends\n",
    "        # ex) The agent drops the pole\n",
    "        if done:\n",
    "            # print the score and break out of the loop\n",
    "            print(\"episode: {}/{}, score: {}\"\n",
    "                  .format(e, episodes, step))\n",
    "            break\n",
    "        # train the agent with the experience of the episode\n",
    "        agent.replay(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=1000000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.training_frequency = 4\n",
    "        self.target_update_frequency = 1000\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.reset_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(48, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def reset_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, total_step, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        if total_step % self.training_frequency == 0:\n",
    "            minibatch = random.sample(self.memory, batch_size)\n",
    "            for state, action, reward, next_state, done in minibatch:\n",
    "                target = reward\n",
    "                if not done:\n",
    "                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\n",
    "                target_f = self.target_model.predict(state)\n",
    "                target_f[0][action] = target\n",
    "                self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "        if step % self.target_update_frequency == 0:\n",
    "            self.reset_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  6\n",
      "Action space:  3\n",
      "episode: 0/5000, score: -6.0\n",
      "episode: 1/5000, score: -6.0\n",
      "episode: 2/5000, score: -6.0\n",
      "episode: 3/5000, score: -6.0\n",
      "episode: 4/5000, score: -4.67\n",
      "episode: 5/5000, score: -6.0\n",
      "episode: 6/5000, score: -6.0\n",
      "episode: 7/5000, score: -6.0\n",
      "episode: 8/5000, score: -6.0\n",
      "episode: 9/5000, score: -6.0\n",
      "episode: 10/5000, score: -4.33\n",
      "episode: 11/5000, score: -6.0\n",
      "episode: 12/5000, score: -6.0\n",
      "episode: 13/5000, score: -6.0\n",
      "episode: 14/5000, score: -3.67\n",
      "episode: 15/5000, score: -2.15\n",
      "episode: 16/5000, score: -6.0\n",
      "episode: 17/5000, score: -4.38\n",
      "episode: 18/5000, score: -6.0\n",
      "episode: 19/5000, score: -6.0\n",
      "episode: 20/5000, score: -6.0\n",
      "episode: 21/5000, score: -6.0\n",
      "episode: 22/5000, score: -6.0\n",
      "episode: 23/5000, score: -6.0\n",
      "episode: 24/5000, score: -6.0\n",
      "episode: 25/5000, score: -6.0\n",
      "episode: 26/5000, score: -6.0\n",
      "episode: 27/5000, score: -6.0\n",
      "episode: 28/5000, score: -4.09\n",
      "episode: 29/5000, score: -1.93\n",
      "episode: 30/5000, score: -3.54\n",
      "episode: 31/5000, score: -6.0\n",
      "episode: 32/5000, score: -6.0\n",
      "episode: 33/5000, score: -6.0\n",
      "episode: 34/5000, score: -6.0\n",
      "episode: 35/5000, score: -6.0\n",
      "episode: 36/5000, score: -6.0\n",
      "episode: 37/5000, score: -6.0\n",
      "episode: 38/5000, score: -6.0\n",
      "episode: 39/5000, score: -6.0\n",
      "episode: 40/5000, score: -2.35\n",
      "episode: 41/5000, score: -6.0\n",
      "episode: 42/5000, score: -6.0\n",
      "episode: 43/5000, score: -6.0\n",
      "episode: 44/5000, score: -6.0\n",
      "episode: 45/5000, score: -2.41\n",
      "episode: 46/5000, score: -0.93\n",
      "episode: 47/5000, score: -3.27\n",
      "episode: 48/5000, score: -6.0\n",
      "episode: 49/5000, score: -6.0\n",
      "episode: 50/5000, score: -6.0\n",
      "episode: 51/5000, score: -6.0\n",
      "episode: 52/5000, score: -6.0\n",
      "episode: 53/5000, score: -6.0\n",
      "episode: 54/5000, score: -2.37\n",
      "episode: 55/5000, score: -4.42\n",
      "episode: 56/5000, score: -2.82\n",
      "episode: 57/5000, score: -2.97\n",
      "episode: 58/5000, score: -6.0\n",
      "episode: 59/5000, score: -6.0\n",
      "episode: 60/5000, score: -6.0\n",
      "episode: 61/5000, score: -3.67\n",
      "episode: 62/5000, score: -6.0\n",
      "episode: 63/5000, score: -6.0\n",
      "episode: 64/5000, score: -6.0\n",
      "episode: 65/5000, score: -6.0\n",
      "episode: 66/5000, score: -6.0\n",
      "episode: 67/5000, score: -6.0\n",
      "episode: 68/5000, score: -6.0\n",
      "episode: 69/5000, score: -6.0\n",
      "episode: 70/5000, score: -6.0\n",
      "episode: 71/5000, score: -6.0\n",
      "episode: 72/5000, score: -6.0\n",
      "episode: 73/5000, score: -6.0\n",
      "episode: 74/5000, score: -6.0\n",
      "episode: 75/5000, score: -4.8\n",
      "episode: 76/5000, score: -6.0\n",
      "episode: 77/5000, score: -6.0\n",
      "episode: 78/5000, score: -6.0\n",
      "episode: 79/5000, score: -6.0\n",
      "episode: 80/5000, score: -6.0\n",
      "episode: 81/5000, score: -6.0\n",
      "episode: 82/5000, score: -6.0\n",
      "episode: 83/5000, score: -6.0\n",
      "episode: 84/5000, score: -6.0\n",
      "episode: 85/5000, score: -6.0\n",
      "episode: 86/5000, score: -6.0\n",
      "episode: 87/5000, score: -6.0\n",
      "episode: 88/5000, score: -6.0\n",
      "episode: 89/5000, score: -6.0\n",
      "episode: 90/5000, score: -6.0\n",
      "episode: 91/5000, score: -6.0\n",
      "episode: 92/5000, score: -6.0\n",
      "episode: 93/5000, score: -6.0\n",
      "episode: 94/5000, score: -6.0\n",
      "episode: 95/5000, score: -6.0\n",
      "episode: 96/5000, score: -6.0\n",
      "episode: 97/5000, score: -6.0\n",
      "episode: 98/5000, score: -6.0\n",
      "episode: 99/5000, score: -6.0\n",
      "episode: 100/5000, score: -6.0\n",
      "episode: 101/5000, score: -6.0\n",
      "episode: 102/5000, score: -6.0\n",
      "episode: 103/5000, score: -6.0\n",
      "episode: 104/5000, score: -3.71\n",
      "episode: 105/5000, score: -3.13\n",
      "episode: 106/5000, score: -6.0\n",
      "episode: 107/5000, score: -6.0\n",
      "episode: 108/5000, score: -6.0\n",
      "episode: 109/5000, score: -4.75\n",
      "episode: 110/5000, score: -6.0\n",
      "episode: 111/5000, score: -6.0\n",
      "episode: 112/5000, score: -1.86\n",
      "episode: 113/5000, score: -6.0\n",
      "episode: 114/5000, score: -3.27\n",
      "episode: 115/5000, score: -6.0\n",
      "episode: 116/5000, score: -6.0\n",
      "episode: 117/5000, score: -6.0\n",
      "episode: 118/5000, score: -2.94\n",
      "episode: 119/5000, score: -6.0\n",
      "episode: 120/5000, score: -6.0\n",
      "episode: 121/5000, score: -4.78\n",
      "episode: 122/5000, score: -4.58\n",
      "episode: 123/5000, score: -6.0\n",
      "episode: 124/5000, score: -3.33\n",
      "episode: 125/5000, score: -3.85\n",
      "episode: 126/5000, score: -6.0\n",
      "episode: 127/5000, score: -6.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubclass_\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: issubclass() arg 1 must be a class",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-53d1fa8840e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# train the agent with the experience of the episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# print the score and break out of the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-f5a21bbcac08>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, total_step, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_begin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_begin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m     95\u001b[0m            \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_t_batch\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3495\u001b[0m     \"\"\"\n\u001b[1;32m   3496\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3497\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3403\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3517\u001b[0m         \u001b[0mkth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3518\u001b[0m     \u001b[0;31m# Check if the array contains any nan's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3519\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3520\u001b[0m         \u001b[0mkth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \"\"\"\n\u001b[0;32m--> 392\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubclass_\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \"\"\"\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize gym environment and the agent\n",
    "env = gym.make('Acrobot-v1')\n",
    "observation_space = env.observation_space.shape[0]\n",
    "print('Observation space: ', observation_space)\n",
    "action_space = env.action_space.n\n",
    "print('Action space: ', action_space)\n",
    "agent = DDQNAgent(observation_space, action_space)\n",
    "episodes = 5000\n",
    "total_step = 0\n",
    "# Iterate the game\n",
    "for e in range(episodes):\n",
    "    # reset state in the beginning of each game\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, observation_space])\n",
    "    # time_t represents each frame of the game\n",
    "    # Our goal is to keep the pole upright as long as possible until score of 500\n",
    "    # the more time_t the more score\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        total_step += 1\n",
    "        # turn this on if you want to render\n",
    "        env.render()\n",
    "        # Decide action\n",
    "        action = agent.act(state)\n",
    "        # Advance the game to the next frame based on the action.\n",
    "        # Reward is 1 for every frame the pole survived\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        reward += score / 100 # increase reward as score accumulates\n",
    "        next_state = np.reshape(next_state, [1, observation_space])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        # train the agent with the experience of the episode\n",
    "        agent.replay(total_step, 32)\n",
    "        if done:\n",
    "            # print the score and break out of the loop\n",
    "            print(\"episode: {}/{}, score: {}\"\n",
    "                  .format(e, episodes, reward))\n",
    "            break\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  8\n",
      "Action space:  4\n",
      "episode: 0/5000, score: -336.6009669325832\n",
      "episode: 1/5000, score: -385.36197845891644\n",
      "episode: 2/5000, score: -351.3034750668105\n",
      "episode: 3/5000, score: -123.72327188997868\n",
      "episode: 4/5000, score: -95.34285632515093\n",
      "episode: 5/5000, score: -338.09572620348797\n",
      "episode: 6/5000, score: -762.4843088810056\n",
      "episode: 7/5000, score: 38.866379114550256\n",
      "episode: 8/5000, score: -116.71856271629143\n",
      "episode: 9/5000, score: -141.89130807992095\n",
      "episode: 10/5000, score: -212.82125611928203\n",
      "episode: 11/5000, score: -91.80675549085578\n",
      "episode: 12/5000, score: -110.49760249660756\n",
      "episode: 13/5000, score: -363.97496912839364\n",
      "episode: 14/5000, score: -119.29596135808976\n",
      "episode: 15/5000, score: -564.6885638376685\n",
      "episode: 16/5000, score: -166.35734838253796\n",
      "episode: 17/5000, score: -341.841953751224\n",
      "episode: 18/5000, score: -242.3747193073728\n",
      "episode: 19/5000, score: -356.01385076245185\n",
      "episode: 20/5000, score: -445.33345961224063\n",
      "episode: 21/5000, score: -458.0842969806739\n",
      "episode: 22/5000, score: -37.609626644381095\n",
      "episode: 23/5000, score: -117.52933273960952\n",
      "episode: 24/5000, score: -111.09523083752818\n",
      "episode: 25/5000, score: -160.35108396841264\n",
      "episode: 26/5000, score: -182.86480572760868\n",
      "episode: 27/5000, score: -46.93134603373576\n",
      "episode: 28/5000, score: -186.6501555255275\n",
      "episode: 29/5000, score: -164.90834629539268\n",
      "episode: 30/5000, score: -162.59328998979646\n",
      "episode: 31/5000, score: -220.06295385965484\n",
      "episode: 32/5000, score: -212.23986301754684\n",
      "episode: 33/5000, score: -373.19730035295237\n",
      "episode: 34/5000, score: -271.00627621572255\n",
      "episode: 35/5000, score: -225.6572239941408\n",
      "episode: 36/5000, score: -287.64366886287974\n",
      "episode: 37/5000, score: -196.11866073714486\n",
      "episode: 38/5000, score: -21.80408886806046\n",
      "episode: 39/5000, score: -495.92829781784576\n",
      "episode: 40/5000, score: -327.993980122204\n",
      "episode: 41/5000, score: -258.7347109557308\n",
      "episode: 42/5000, score: -528.7416096526867\n",
      "episode: 43/5000, score: -228.74536248210842\n",
      "episode: 44/5000, score: -328.8897318412301\n",
      "episode: 45/5000, score: -516.2099994933121\n",
      "episode: 46/5000, score: -197.96792418798316\n",
      "episode: 47/5000, score: -409.2468962645247\n",
      "episode: 48/5000, score: -274.5329885454839\n",
      "episode: 49/5000, score: -359.6975620167058\n",
      "episode: 50/5000, score: -452.8625783526836\n",
      "episode: 51/5000, score: -195.55365986876456\n",
      "episode: 52/5000, score: -337.33915506557224\n",
      "episode: 53/5000, score: -482.7448505026631\n",
      "episode: 54/5000, score: -284.0610263679642\n",
      "episode: 55/5000, score: -356.64173533360656\n",
      "episode: 56/5000, score: -361.70635139903084\n",
      "episode: 57/5000, score: -521.5928866124082\n",
      "episode: 58/5000, score: -513.9244528240533\n",
      "episode: 59/5000, score: -501.9971017993281\n",
      "episode: 60/5000, score: -332.4855309052351\n",
      "episode: 61/5000, score: -540.5570817932503\n",
      "episode: 62/5000, score: -551.7877534587884\n",
      "episode: 63/5000, score: -237.55938847777188\n",
      "episode: 64/5000, score: -280.12282421394735\n",
      "episode: 65/5000, score: -168.4336305188515\n",
      "episode: 66/5000, score: -210.86079293557418\n",
      "episode: 67/5000, score: -378.50867243171433\n",
      "episode: 68/5000, score: -92.85859968496888\n",
      "episode: 69/5000, score: -328.4290438785072\n",
      "episode: 70/5000, score: -20.941178763295596\n",
      "episode: 71/5000, score: -225.60406986010685\n",
      "episode: 72/5000, score: -621.8694704012727\n",
      "episode: 73/5000, score: -531.1876175948422\n",
      "episode: 74/5000, score: -339.0125812614847\n",
      "episode: 75/5000, score: -142.95369954081283\n",
      "episode: 76/5000, score: -472.4301115319843\n",
      "episode: 77/5000, score: -358.45281926930136\n",
      "episode: 78/5000, score: -386.85006948778374\n",
      "episode: 79/5000, score: -301.51715944178454\n",
      "episode: 80/5000, score: -248.82734397121655\n",
      "episode: 81/5000, score: -297.6605293564543\n",
      "episode: 82/5000, score: -319.97562654390543\n",
      "episode: 83/5000, score: -279.6245894786175\n",
      "episode: 84/5000, score: -497.37560263643064\n",
      "episode: 85/5000, score: -244.86857234785816\n",
      "episode: 86/5000, score: -490.5437234214995\n",
      "episode: 87/5000, score: -395.9290790036046\n",
      "episode: 88/5000, score: -445.99195382708064\n",
      "episode: 89/5000, score: -540.6613571415709\n",
      "episode: 90/5000, score: -463.1913621165039\n",
      "episode: 91/5000, score: -458.45498708880575\n",
      "episode: 92/5000, score: -357.12229137633506\n",
      "episode: 93/5000, score: -311.95632247232163\n",
      "episode: 94/5000, score: -308.6158521705175\n",
      "episode: 95/5000, score: -354.44613598271417\n",
      "episode: 96/5000, score: -222.97668204672215\n",
      "episode: 97/5000, score: -188.91482320039597\n",
      "episode: 98/5000, score: -356.5831559997725\n",
      "episode: 99/5000, score: -314.67743277442105\n",
      "episode: 100/5000, score: -437.4315046893982\n",
      "episode: 101/5000, score: -286.19298059378866\n",
      "episode: 102/5000, score: -256.75330341178056\n",
      "episode: 103/5000, score: -203.50716136499824\n",
      "episode: 104/5000, score: -222.31108855456478\n",
      "episode: 105/5000, score: -157.6122592684141\n",
      "episode: 106/5000, score: -219.63953962273337\n",
      "episode: 107/5000, score: -482.4751128036934\n",
      "episode: 108/5000, score: -272.784451780237\n",
      "episode: 109/5000, score: -142.31222539497844\n",
      "episode: 110/5000, score: -314.93487063871873\n",
      "episode: 111/5000, score: -391.25045108246684\n",
      "episode: 112/5000, score: -241.54560876137342\n",
      "episode: 113/5000, score: -289.196261881468\n",
      "episode: 114/5000, score: -288.83686310523143\n",
      "episode: 115/5000, score: -184.08269376180544\n",
      "episode: 116/5000, score: -379.4474252317205\n",
      "episode: 117/5000, score: -434.7353646173487\n",
      "episode: 118/5000, score: -413.4690045573967\n",
      "episode: 119/5000, score: -383.82831546719586\n",
      "episode: 120/5000, score: -438.18874247596125\n",
      "episode: 121/5000, score: -42.76335787316859\n",
      "episode: 122/5000, score: -347.5878114681419\n",
      "episode: 123/5000, score: -277.6179894224451\n",
      "episode: 124/5000, score: -379.2184500397186\n",
      "episode: 125/5000, score: -435.6058279095611\n",
      "episode: 126/5000, score: -205.05849803038248\n",
      "episode: 127/5000, score: -430.4335507386595\n",
      "episode: 128/5000, score: -226.11734811303077\n",
      "episode: 129/5000, score: -222.41636847798878\n",
      "episode: 130/5000, score: -289.9582956059559\n",
      "episode: 131/5000, score: -55.422093153149966\n",
      "episode: 132/5000, score: -405.3000894457811\n",
      "episode: 133/5000, score: -394.84935722998387\n",
      "episode: 134/5000, score: -252.05683178015644\n",
      "episode: 135/5000, score: -297.298761969784\n",
      "episode: 136/5000, score: -306.57532937175654\n",
      "episode: 137/5000, score: -167.6265909169938\n",
      "episode: 138/5000, score: -373.7027620657074\n",
      "episode: 139/5000, score: -264.1131574123589\n",
      "episode: 140/5000, score: 39.57893775833881\n",
      "episode: 141/5000, score: -176.2200637778935\n",
      "episode: 142/5000, score: -296.8746874428077\n",
      "episode: 143/5000, score: -533.6839181889102\n",
      "episode: 144/5000, score: -255.07632800935477\n",
      "episode: 145/5000, score: -434.4047532351358\n",
      "episode: 146/5000, score: 4.972915637079623\n",
      "episode: 147/5000, score: -187.8596218338296\n",
      "episode: 148/5000, score: -206.62893776888694\n",
      "episode: 149/5000, score: -592.2120630873255\n",
      "episode: 150/5000, score: -146.55308738874146\n",
      "episode: 151/5000, score: -126.81000869208958\n",
      "episode: 152/5000, score: -310.6347238459482\n",
      "episode: 153/5000, score: -451.40002066775236\n",
      "episode: 154/5000, score: -506.05840937508555\n",
      "episode: 155/5000, score: -132.71402289694578\n",
      "episode: 156/5000, score: -274.02665272894126\n",
      "episode: 157/5000, score: -462.7129700267882\n",
      "episode: 158/5000, score: -341.8864792663453\n",
      "episode: 159/5000, score: -278.4081935980107\n",
      "episode: 160/5000, score: -279.96176092097437\n",
      "episode: 161/5000, score: -453.0051507680606\n",
      "episode: 162/5000, score: -348.04905487618714\n",
      "episode: 163/5000, score: -367.5027815732448\n",
      "episode: 164/5000, score: -554.5651462874973\n",
      "episode: 165/5000, score: -177.01190358762244\n",
      "episode: 166/5000, score: -446.22726754964725\n",
      "episode: 167/5000, score: -375.2028517575296\n",
      "episode: 168/5000, score: -469.3705174656497\n",
      "episode: 169/5000, score: -152.38210533080508\n",
      "episode: 170/5000, score: -181.53532895361923\n",
      "episode: 171/5000, score: -397.26184523317426\n",
      "episode: 172/5000, score: -298.53542713726245\n",
      "episode: 173/5000, score: -573.3408180962457\n",
      "episode: 174/5000, score: -510.1922808116004\n",
      "episode: 175/5000, score: -475.48973330228506\n",
      "episode: 176/5000, score: -266.4679948733251\n",
      "episode: 177/5000, score: -508.70907765885295\n",
      "episode: 178/5000, score: -336.86790980504105\n",
      "episode: 179/5000, score: -345.28842115654817\n",
      "episode: 180/5000, score: -167.8295504754958\n",
      "episode: 181/5000, score: -582.3836798539412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 182/5000, score: 2.1968779947806354\n",
      "episode: 183/5000, score: -395.54499478811124\n",
      "episode: 184/5000, score: -300.01604451670164\n",
      "episode: 185/5000, score: -488.78968552609444\n",
      "episode: 186/5000, score: -313.8069915451066\n",
      "episode: 187/5000, score: -161.46554659991244\n",
      "episode: 188/5000, score: -97.4372896064249\n",
      "episode: 189/5000, score: -386.0703062349373\n",
      "episode: 190/5000, score: -199.57258020588992\n",
      "episode: 191/5000, score: -406.5390394723177\n",
      "episode: 192/5000, score: -444.2800854637708\n",
      "episode: 193/5000, score: -236.1841440435443\n",
      "episode: 194/5000, score: -434.65724655146585\n",
      "episode: 195/5000, score: -376.50611133071146\n",
      "episode: 196/5000, score: -379.25526837434796\n",
      "episode: 197/5000, score: -515.5869998764756\n",
      "episode: 198/5000, score: -196.47035506724032\n",
      "episode: 199/5000, score: -307.2988270067148\n",
      "episode: 200/5000, score: -212.70651601635763\n",
      "episode: 201/5000, score: -316.02035374067447\n",
      "episode: 202/5000, score: -260.20069934401033\n",
      "episode: 203/5000, score: -235.21801444725136\n",
      "episode: 204/5000, score: -140.8958570593695\n",
      "episode: 205/5000, score: -186.1286623265824\n",
      "episode: 206/5000, score: -467.3333908375002\n",
      "episode: 207/5000, score: -120.85774645826454\n",
      "episode: 208/5000, score: -261.14504363076935\n",
      "episode: 209/5000, score: -217.8048093693721\n",
      "episode: 210/5000, score: -91.50939343164217\n",
      "episode: 211/5000, score: -390.08489864688477\n",
      "episode: 212/5000, score: -205.47165287453808\n",
      "episode: 213/5000, score: -415.4082080697328\n",
      "episode: 214/5000, score: -125.58920716868298\n",
      "episode: 215/5000, score: -124.79883025006109\n",
      "episode: 216/5000, score: 239.78859133842641\n",
      "episode: 217/5000, score: -351.7918804960042\n",
      "episode: 218/5000, score: -270.86388770050377\n",
      "episode: 219/5000, score: -209.47373781345436\n",
      "episode: 220/5000, score: -354.27822728278545\n",
      "episode: 221/5000, score: -144.71646815821504\n",
      "episode: 222/5000, score: -207.62806636280965\n",
      "episode: 223/5000, score: -231.88441478545633\n",
      "episode: 224/5000, score: -223.4545084233344\n",
      "episode: 225/5000, score: -322.41696114149795\n",
      "episode: 226/5000, score: -86.38881387962444\n",
      "episode: 227/5000, score: -491.16795533928547\n",
      "episode: 228/5000, score: -67.18596342916217\n",
      "episode: 229/5000, score: -396.26265793968486\n",
      "episode: 230/5000, score: -152.91177321318463\n",
      "episode: 231/5000, score: -185.60207252707153\n",
      "episode: 232/5000, score: -596.424721577197\n",
      "episode: 233/5000, score: -285.94899291281683\n",
      "episode: 234/5000, score: -232.52303796152862\n",
      "episode: 235/5000, score: -158.87367764850853\n",
      "episode: 236/5000, score: -301.4972265115084\n",
      "episode: 237/5000, score: -459.7327212529177\n",
      "episode: 238/5000, score: -300.78661980853917\n",
      "episode: 239/5000, score: -298.47111696695913\n",
      "episode: 240/5000, score: -134.88791434682773\n",
      "episode: 241/5000, score: -376.3183989255294\n",
      "episode: 242/5000, score: -41.463408183674396\n",
      "episode: 243/5000, score: -136.24159304325883\n",
      "episode: 244/5000, score: -193.18543582043645\n",
      "episode: 245/5000, score: -593.645533682997\n",
      "episode: 246/5000, score: -242.07379217158788\n",
      "episode: 247/5000, score: -535.1086448989822\n",
      "episode: 248/5000, score: -375.144774398663\n",
      "episode: 249/5000, score: -394.66184594631346\n",
      "episode: 250/5000, score: -472.7873851617903\n",
      "episode: 251/5000, score: -192.3278626607135\n",
      "episode: 252/5000, score: -159.17736847034303\n",
      "episode: 253/5000, score: -98.72341032428602\n",
      "episode: 254/5000, score: -188.8082632553131\n",
      "episode: 255/5000, score: -244.44503248054198\n",
      "episode: 256/5000, score: -203.9090633691373\n",
      "episode: 257/5000, score: -98.88416200908692\n",
      "episode: 258/5000, score: -216.09926804175524\n",
      "episode: 259/5000, score: -320.26530244023235\n",
      "episode: 260/5000, score: -530.9554482679673\n",
      "episode: 261/5000, score: -168.61694881022433\n",
      "episode: 262/5000, score: -293.25998069078366\n",
      "episode: 263/5000, score: -286.7925159672942\n",
      "episode: 264/5000, score: -167.20152826022075\n",
      "episode: 265/5000, score: -353.23649547215234\n",
      "episode: 266/5000, score: -233.5066198791522\n",
      "episode: 267/5000, score: -54.31370758743148\n",
      "episode: 268/5000, score: -133.49390477871222\n",
      "episode: 269/5000, score: -109.96397926327805\n",
      "episode: 270/5000, score: -136.41485008523017\n",
      "episode: 271/5000, score: -660.0106580626675\n",
      "episode: 272/5000, score: -116.23458089550962\n",
      "episode: 273/5000, score: -172.37082082559\n",
      "episode: 274/5000, score: -26.662524346200257\n",
      "episode: 275/5000, score: -211.23018308401265\n",
      "episode: 276/5000, score: -91.87684273031677\n",
      "episode: 277/5000, score: -164.0181736179009\n",
      "episode: 278/5000, score: -273.33265734393547\n",
      "episode: 279/5000, score: -378.51309025291454\n",
      "episode: 280/5000, score: -413.36988076874\n",
      "episode: 281/5000, score: -294.445380833514\n",
      "episode: 282/5000, score: -83.1570281312998\n",
      "episode: 283/5000, score: -153.30223882378246\n",
      "episode: 284/5000, score: -500.5329052287967\n",
      "episode: 285/5000, score: -561.1543060024694\n",
      "episode: 286/5000, score: 2.8683583998724203\n",
      "episode: 287/5000, score: -373.6092848419571\n",
      "episode: 288/5000, score: -744.9945171499774\n",
      "episode: 289/5000, score: -485.63657832866676\n",
      "episode: 290/5000, score: -56.17506172135279\n",
      "episode: 291/5000, score: -387.36236077430556\n",
      "episode: 292/5000, score: -39.40307811120981\n",
      "episode: 293/5000, score: -124.1124058164244\n",
      "episode: 294/5000, score: -213.58999217215575\n",
      "episode: 295/5000, score: -131.52954565810603\n",
      "episode: 296/5000, score: -186.1921252598462\n",
      "episode: 297/5000, score: -329.28746068206624\n",
      "episode: 298/5000, score: -196.0369501825922\n",
      "episode: 299/5000, score: -522.950366628372\n",
      "episode: 300/5000, score: -608.9803993039955\n",
      "episode: 301/5000, score: -249.3969024141151\n",
      "episode: 302/5000, score: -330.46251588921655\n",
      "episode: 303/5000, score: -272.2825083399706\n",
      "episode: 304/5000, score: -317.18615648540003\n",
      "episode: 305/5000, score: -276.67563156554036\n",
      "episode: 306/5000, score: -524.0443628180395\n",
      "episode: 307/5000, score: -210.97199373584192\n",
      "episode: 308/5000, score: -109.53855335372211\n",
      "episode: 309/5000, score: -220.2785298585726\n",
      "episode: 310/5000, score: -231.5358374353343\n",
      "episode: 311/5000, score: -115.68827664223377\n",
      "episode: 312/5000, score: -470.8074437198229\n",
      "episode: 313/5000, score: -159.3807155329862\n",
      "episode: 314/5000, score: -331.7800371743548\n",
      "episode: 315/5000, score: -167.6898089200337\n",
      "episode: 316/5000, score: -205.66781631779025\n",
      "episode: 317/5000, score: -353.54107786900573\n",
      "episode: 318/5000, score: -374.4939168184638\n",
      "episode: 319/5000, score: -285.4046576722193\n",
      "episode: 320/5000, score: -179.35653158876778\n",
      "episode: 321/5000, score: -90.70787098043938\n",
      "episode: 322/5000, score: -193.91198541822504\n",
      "episode: 323/5000, score: -127.77516701903832\n",
      "episode: 324/5000, score: -404.22555882857705\n",
      "episode: 325/5000, score: -477.34689107970394\n",
      "episode: 326/5000, score: -331.9038211781385\n",
      "episode: 327/5000, score: -302.8467535207013\n",
      "episode: 328/5000, score: -198.2058652459836\n",
      "episode: 329/5000, score: -484.0715242792111\n",
      "episode: 330/5000, score: -356.3309725149609\n",
      "episode: 331/5000, score: -299.19800053411836\n",
      "episode: 332/5000, score: -175.81389691504586\n",
      "episode: 333/5000, score: -330.6992920560082\n",
      "episode: 334/5000, score: -222.13509365881572\n",
      "episode: 335/5000, score: -246.7691747263804\n",
      "episode: 336/5000, score: -106.37606302525407\n",
      "episode: 337/5000, score: -53.31429179425808\n",
      "episode: 338/5000, score: -328.19781350204426\n",
      "episode: 339/5000, score: -335.0366937794413\n",
      "episode: 340/5000, score: -585.7169773582534\n",
      "episode: 341/5000, score: -207.7003281771047\n",
      "episode: 342/5000, score: -367.3792939430207\n",
      "episode: 343/5000, score: -511.14952827032795\n",
      "episode: 344/5000, score: -260.5653536719375\n",
      "episode: 345/5000, score: 5.518260180993664\n",
      "episode: 346/5000, score: -50.732562712791264\n",
      "episode: 347/5000, score: -230.16108924907985\n",
      "episode: 348/5000, score: -122.4884863835093\n",
      "episode: 349/5000, score: -226.83502682551716\n",
      "episode: 350/5000, score: -183.45356076690325\n",
      "episode: 351/5000, score: -73.36275562860143\n",
      "episode: 352/5000, score: -285.1384078277607\n",
      "episode: 353/5000, score: -268.5880707541467\n",
      "episode: 354/5000, score: -72.00787275304111\n",
      "episode: 355/5000, score: -272.00567240415114\n",
      "episode: 356/5000, score: -227.54151600901136\n",
      "episode: 357/5000, score: -459.520196706992\n",
      "episode: 358/5000, score: -200.9323215025581\n",
      "episode: 359/5000, score: -198.1835017308573\n",
      "episode: 360/5000, score: -275.37016876824606\n",
      "episode: 361/5000, score: -345.6624092412491\n",
      "episode: 362/5000, score: -413.4404692299559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 363/5000, score: -199.75857063331736\n",
      "episode: 364/5000, score: -287.0444362877013\n",
      "episode: 365/5000, score: -386.34255243084556\n",
      "episode: 366/5000, score: -49.37409385212102\n",
      "episode: 367/5000, score: -146.72888670378956\n",
      "episode: 368/5000, score: -105.70157325315829\n",
      "episode: 369/5000, score: -131.7202076025022\n",
      "episode: 370/5000, score: -466.24837147383835\n",
      "episode: 371/5000, score: -110.04283682936551\n",
      "episode: 372/5000, score: -519.9298894726817\n",
      "episode: 373/5000, score: -325.88547444970067\n",
      "episode: 374/5000, score: -188.88818369541994\n",
      "episode: 375/5000, score: -413.68969999307114\n",
      "episode: 376/5000, score: -457.3269146824696\n",
      "episode: 377/5000, score: -221.34858445855986\n",
      "episode: 378/5000, score: -442.94695354628965\n",
      "episode: 379/5000, score: -333.9039121065436\n",
      "episode: 380/5000, score: -613.1834088582093\n",
      "episode: 381/5000, score: -545.7356979674042\n",
      "episode: 382/5000, score: -511.19227205627664\n",
      "episode: 383/5000, score: -385.78860682093557\n",
      "episode: 384/5000, score: -52.15634779959126\n",
      "episode: 385/5000, score: -342.3310958574282\n",
      "episode: 386/5000, score: -430.9168690093467\n",
      "episode: 387/5000, score: -287.8820017487467\n",
      "episode: 388/5000, score: -382.0650140124008\n",
      "episode: 389/5000, score: -98.15169246524518\n",
      "episode: 390/5000, score: -479.9013229147458\n",
      "episode: 391/5000, score: -180.7639966395684\n",
      "episode: 392/5000, score: -360.17417754248856\n",
      "episode: 393/5000, score: -509.1740475973733\n",
      "episode: 394/5000, score: -406.9039057800386\n",
      "episode: 395/5000, score: -400.9206469133142\n",
      "episode: 396/5000, score: -71.28264374441068\n",
      "episode: 397/5000, score: -102.72947927264545\n",
      "episode: 398/5000, score: -173.13816973749624\n",
      "episode: 399/5000, score: -162.79523688354337\n",
      "episode: 400/5000, score: -561.4468547611397\n",
      "episode: 401/5000, score: -193.96152018572405\n",
      "episode: 402/5000, score: -177.53840692681223\n",
      "episode: 403/5000, score: -391.0174911546177\n",
      "episode: 404/5000, score: -365.7350946715492\n",
      "episode: 405/5000, score: -151.18579908925622\n",
      "episode: 406/5000, score: -171.49753605288043\n",
      "episode: 407/5000, score: -359.64017820841406\n",
      "episode: 408/5000, score: -135.5768281895804\n",
      "episode: 409/5000, score: -233.29537014054216\n",
      "episode: 410/5000, score: -137.5801560996383\n",
      "episode: 411/5000, score: -199.96830290119095\n",
      "episode: 412/5000, score: -376.7783290625815\n",
      "episode: 413/5000, score: -202.75319434438504\n",
      "episode: 414/5000, score: -284.55428974877344\n",
      "episode: 415/5000, score: -320.3517938328489\n",
      "episode: 416/5000, score: -200.79588760688281\n",
      "episode: 417/5000, score: -295.75182311115805\n",
      "episode: 418/5000, score: -190.65342349640835\n",
      "episode: 419/5000, score: -162.06661917874823\n",
      "episode: 420/5000, score: -217.6609265321305\n",
      "episode: 421/5000, score: -174.32527091528104\n",
      "episode: 422/5000, score: -218.15465162870743\n",
      "episode: 423/5000, score: -219.28737941705432\n",
      "episode: 424/5000, score: -380.7894145086729\n",
      "episode: 425/5000, score: -494.3515738477198\n",
      "episode: 426/5000, score: -101.05264279521776\n",
      "episode: 427/5000, score: -553.5871647115414\n",
      "episode: 428/5000, score: -171.89353312305957\n",
      "episode: 429/5000, score: -248.1624069737356\n",
      "episode: 430/5000, score: -116.86514373427087\n",
      "episode: 431/5000, score: -519.6206788797765\n",
      "episode: 432/5000, score: -512.1435024748746\n",
      "episode: 433/5000, score: -161.5474593094125\n",
      "episode: 434/5000, score: -227.31266991221148\n",
      "episode: 435/5000, score: -380.69413807298645\n",
      "episode: 436/5000, score: -347.0999133335186\n",
      "episode: 437/5000, score: -513.0568537131559\n",
      "episode: 438/5000, score: -87.48512849465209\n",
      "episode: 439/5000, score: -373.79361418168327\n",
      "episode: 440/5000, score: -111.32346397859149\n",
      "episode: 441/5000, score: -273.5149523445363\n",
      "episode: 442/5000, score: -296.69004965179795\n",
      "episode: 443/5000, score: -275.49846439540454\n",
      "episode: 444/5000, score: -386.1408358478126\n",
      "episode: 445/5000, score: -401.5021791782595\n",
      "episode: 446/5000, score: -325.24674656879426\n",
      "episode: 447/5000, score: -240.54577376677466\n",
      "episode: 448/5000, score: -160.87442876669974\n",
      "episode: 449/5000, score: -311.0408433159382\n",
      "episode: 450/5000, score: -138.9297014022656\n",
      "episode: 451/5000, score: -194.15653182725512\n",
      "episode: 452/5000, score: -131.19871288715342\n",
      "episode: 453/5000, score: -287.5975250392182\n",
      "episode: 454/5000, score: -114.64199258088394\n",
      "episode: 455/5000, score: -196.13934015486277\n",
      "episode: 456/5000, score: -353.88990456862757\n",
      "episode: 457/5000, score: -122.4064496335699\n",
      "episode: 458/5000, score: -372.28615940019176\n",
      "episode: 459/5000, score: -310.6894595234173\n",
      "episode: 460/5000, score: -320.74360125324995\n",
      "episode: 461/5000, score: -330.1647709800483\n",
      "episode: 462/5000, score: -422.8562414630294\n",
      "episode: 463/5000, score: -328.2443174057463\n",
      "episode: 464/5000, score: -390.32671975917884\n",
      "episode: 465/5000, score: -305.82512017163356\n",
      "episode: 466/5000, score: -344.46724370858425\n",
      "episode: 467/5000, score: -400.8857724442947\n",
      "episode: 468/5000, score: -225.77441967307675\n",
      "episode: 469/5000, score: -98.41390900206068\n",
      "episode: 470/5000, score: -363.51721233088074\n",
      "episode: 471/5000, score: -322.3763362282581\n",
      "episode: 472/5000, score: -351.0659561928656\n",
      "episode: 473/5000, score: -407.04394363096\n",
      "episode: 474/5000, score: 11.696876001394287\n",
      "episode: 475/5000, score: -262.52451582438886\n",
      "episode: 476/5000, score: -411.71422834864\n",
      "episode: 477/5000, score: -559.4401040075752\n",
      "episode: 478/5000, score: -380.7306231700359\n",
      "episode: 479/5000, score: -409.12677984284056\n",
      "episode: 480/5000, score: -269.4859725659213\n",
      "episode: 481/5000, score: -434.4802916654415\n",
      "episode: 482/5000, score: -490.52247716062266\n",
      "episode: 483/5000, score: -474.7892196413699\n",
      "episode: 484/5000, score: -203.12010474810273\n",
      "episode: 485/5000, score: -226.22286501126257\n",
      "episode: 486/5000, score: -332.49978001399614\n",
      "episode: 487/5000, score: -327.3402022107162\n",
      "episode: 488/5000, score: -314.9489618492563\n",
      "episode: 489/5000, score: -368.6541663544781\n",
      "episode: 490/5000, score: -354.088551057324\n",
      "episode: 491/5000, score: -418.5100408065413\n",
      "episode: 492/5000, score: -446.7930044688375\n",
      "episode: 493/5000, score: -283.1117931370397\n",
      "episode: 494/5000, score: -544.928322735921\n",
      "episode: 495/5000, score: -546.2882538720833\n",
      "episode: 496/5000, score: -152.64245418718457\n",
      "episode: 497/5000, score: -337.03518213956033\n",
      "episode: 498/5000, score: -210.34742453856086\n",
      "episode: 499/5000, score: -323.1543412564573\n",
      "episode: 500/5000, score: -237.65567251190032\n",
      "episode: 501/5000, score: -319.341224738961\n",
      "episode: 502/5000, score: -191.2199873530542\n",
      "episode: 503/5000, score: -605.5523153573891\n",
      "episode: 504/5000, score: -168.09761164859307\n",
      "episode: 505/5000, score: -648.3923919476241\n",
      "episode: 506/5000, score: -462.9852576252658\n",
      "episode: 507/5000, score: -218.0686875595878\n",
      "episode: 508/5000, score: -364.1534349341362\n",
      "episode: 509/5000, score: -375.92000057797867\n",
      "episode: 510/5000, score: -288.1849662617316\n",
      "episode: 511/5000, score: -333.84881637165716\n",
      "episode: 512/5000, score: -213.19949520044736\n",
      "episode: 513/5000, score: -143.53261716980703\n",
      "episode: 514/5000, score: -338.7952493419725\n",
      "episode: 515/5000, score: -333.12202447996225\n",
      "episode: 516/5000, score: -362.58707673420497\n",
      "episode: 517/5000, score: -428.3719084563938\n",
      "episode: 518/5000, score: -181.92459607966234\n",
      "episode: 519/5000, score: -341.34648647735816\n",
      "episode: 520/5000, score: -195.75015265562425\n",
      "episode: 521/5000, score: -326.8003055935542\n",
      "episode: 522/5000, score: -84.02891408068086\n",
      "episode: 523/5000, score: -209.45988908812114\n",
      "episode: 524/5000, score: -281.7378951731727\n",
      "episode: 525/5000, score: -332.7582686362519\n",
      "episode: 526/5000, score: -343.4803815629601\n",
      "episode: 527/5000, score: -296.90365813554865\n",
      "episode: 528/5000, score: -107.61635751124658\n",
      "episode: 529/5000, score: -420.60586766146673\n",
      "episode: 530/5000, score: -181.7758524092074\n",
      "episode: 531/5000, score: -566.9688402539298\n",
      "episode: 532/5000, score: -182.42820810580508\n",
      "episode: 533/5000, score: -166.26303529871947\n",
      "episode: 534/5000, score: -432.47515051924273\n",
      "episode: 535/5000, score: -300.66871031816015\n",
      "episode: 536/5000, score: -171.32164362293054\n",
      "episode: 537/5000, score: -151.00706356968652\n",
      "episode: 538/5000, score: -291.26117704365856\n",
      "episode: 539/5000, score: -177.2542008128493\n",
      "episode: 540/5000, score: -336.9080375714856\n",
      "episode: 541/5000, score: -468.6874503839943\n",
      "episode: 542/5000, score: -195.75433742847287\n",
      "episode: 543/5000, score: -270.27795167192266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 544/5000, score: -270.378595438664\n",
      "episode: 545/5000, score: -415.1669751606499\n",
      "episode: 546/5000, score: -729.6292575982134\n",
      "episode: 547/5000, score: -115.59315600817686\n",
      "episode: 548/5000, score: -212.1602919737211\n",
      "episode: 549/5000, score: -245.34790261319839\n",
      "episode: 550/5000, score: -458.9431761085586\n",
      "episode: 551/5000, score: -422.910050471436\n",
      "episode: 552/5000, score: -408.24680490072205\n",
      "episode: 553/5000, score: -328.79587342368404\n",
      "episode: 554/5000, score: -244.5949583273988\n",
      "episode: 555/5000, score: -274.89346204557785\n",
      "episode: 556/5000, score: -188.17749733947537\n",
      "episode: 557/5000, score: -119.45225562560239\n",
      "episode: 558/5000, score: -353.2404986337332\n",
      "episode: 559/5000, score: -242.3788694281758\n",
      "episode: 560/5000, score: -455.02302947041073\n",
      "episode: 561/5000, score: -423.58565663031106\n",
      "episode: 562/5000, score: -264.26680774850377\n",
      "episode: 563/5000, score: -366.52799509707955\n",
      "episode: 564/5000, score: -420.79681869805665\n",
      "episode: 565/5000, score: -259.49574438987327\n",
      "episode: 566/5000, score: -371.50981461233476\n",
      "episode: 567/5000, score: -295.4766290756703\n",
      "episode: 568/5000, score: -224.08565434771472\n",
      "episode: 569/5000, score: -456.18305802378615\n",
      "episode: 570/5000, score: -234.82110021763248\n",
      "episode: 571/5000, score: -425.6322477043911\n",
      "episode: 572/5000, score: -340.1353456605134\n",
      "episode: 573/5000, score: -519.3910473355479\n",
      "episode: 574/5000, score: -402.33901469349377\n",
      "episode: 575/5000, score: -392.90421518595065\n",
      "episode: 576/5000, score: -327.7748098287302\n",
      "episode: 577/5000, score: -353.3496924579289\n",
      "episode: 578/5000, score: -301.52167281239105\n",
      "episode: 579/5000, score: -253.97367201457445\n",
      "episode: 580/5000, score: -91.10525143662507\n",
      "episode: 581/5000, score: -206.10574743584965\n",
      "episode: 582/5000, score: -153.50129158014718\n",
      "episode: 583/5000, score: -228.33572122212166\n",
      "episode: 584/5000, score: -110.69757260236031\n",
      "episode: 585/5000, score: -163.25730934163062\n",
      "episode: 586/5000, score: -317.4324652576676\n",
      "episode: 587/5000, score: -384.50676959294196\n",
      "episode: 588/5000, score: -453.3064992367543\n",
      "episode: 589/5000, score: -261.52065836066004\n",
      "episode: 590/5000, score: -246.88378647537454\n",
      "episode: 591/5000, score: -90.58374973023271\n",
      "episode: 592/5000, score: -349.49182858902105\n",
      "episode: 593/5000, score: -326.39670356875786\n",
      "episode: 594/5000, score: -169.20773314312342\n",
      "episode: 595/5000, score: -171.62497658609263\n",
      "episode: 596/5000, score: -329.9794697196427\n",
      "episode: 597/5000, score: -286.74813596769616\n",
      "episode: 598/5000, score: -229.03555430575892\n",
      "episode: 599/5000, score: -162.47270311022072\n",
      "episode: 600/5000, score: -254.24247460178984\n",
      "episode: 601/5000, score: -549.9925203328849\n",
      "episode: 602/5000, score: -304.8130272543596\n",
      "episode: 603/5000, score: -127.23790024439384\n",
      "episode: 604/5000, score: -153.74517946555028\n",
      "episode: 605/5000, score: -318.010700743344\n",
      "episode: 606/5000, score: -128.9805343330143\n",
      "episode: 607/5000, score: -297.3163700447193\n",
      "episode: 608/5000, score: -141.56297070170913\n",
      "episode: 609/5000, score: -237.12703102335013\n",
      "episode: 610/5000, score: -506.8691807108267\n",
      "episode: 611/5000, score: -374.2378293028822\n",
      "episode: 612/5000, score: -196.64175338248913\n",
      "episode: 613/5000, score: -278.5982489355653\n",
      "episode: 614/5000, score: -305.8770229997653\n",
      "episode: 615/5000, score: -130.35721145371667\n",
      "episode: 616/5000, score: -268.4570833190263\n",
      "episode: 617/5000, score: -235.02840429386413\n",
      "episode: 618/5000, score: -297.1212237893408\n",
      "episode: 619/5000, score: -282.2415478468598\n",
      "episode: 620/5000, score: -103.10011600714682\n",
      "episode: 621/5000, score: -335.6321690161358\n",
      "episode: 622/5000, score: -165.77920206149844\n",
      "episode: 623/5000, score: -220.99378156225248\n",
      "episode: 624/5000, score: -180.24198019246\n",
      "episode: 625/5000, score: -269.4217503341492\n",
      "episode: 626/5000, score: -636.8060677864559\n",
      "episode: 627/5000, score: -258.0181750404399\n",
      "episode: 628/5000, score: -76.15773009998526\n",
      "episode: 629/5000, score: -299.9539673999561\n",
      "episode: 630/5000, score: -170.82375282642658\n",
      "episode: 631/5000, score: -90.50507724802287\n",
      "episode: 632/5000, score: -303.609739523616\n",
      "episode: 633/5000, score: -478.6405749098079\n",
      "episode: 634/5000, score: -246.40110977431584\n",
      "episode: 635/5000, score: -78.5821910753279\n",
      "episode: 636/5000, score: -112.7869479881118\n",
      "episode: 637/5000, score: -76.22200435664973\n",
      "episode: 638/5000, score: -293.032750925438\n",
      "episode: 639/5000, score: -329.6999282567108\n",
      "episode: 640/5000, score: -329.51923210145355\n",
      "episode: 641/5000, score: -561.9840380369844\n",
      "episode: 642/5000, score: -407.4453136330389\n",
      "episode: 643/5000, score: 1.2419972611197068\n",
      "episode: 644/5000, score: -146.81262083664558\n",
      "episode: 645/5000, score: -188.8330131266174\n",
      "episode: 646/5000, score: -347.92896092974667\n",
      "episode: 647/5000, score: -62.592638956223595\n",
      "episode: 648/5000, score: -82.51065247175498\n",
      "episode: 649/5000, score: -404.9864652954187\n",
      "episode: 650/5000, score: -348.97159838230175\n",
      "episode: 651/5000, score: -297.1188957071439\n",
      "episode: 652/5000, score: -218.4637245368607\n",
      "episode: 653/5000, score: -179.40404055338192\n",
      "episode: 654/5000, score: -264.7726020869195\n",
      "episode: 655/5000, score: -399.5968661691654\n",
      "episode: 656/5000, score: -176.99417480258782\n",
      "episode: 657/5000, score: -21.978292015389485\n",
      "episode: 658/5000, score: -298.7004491123848\n",
      "episode: 659/5000, score: -211.57204113703582\n",
      "episode: 660/5000, score: -352.1401712366837\n",
      "episode: 661/5000, score: -429.9200574350819\n",
      "episode: 662/5000, score: -278.8340859008583\n",
      "episode: 663/5000, score: -352.0735179064357\n",
      "episode: 664/5000, score: -354.72265554649323\n",
      "episode: 665/5000, score: -162.0447934781015\n",
      "episode: 666/5000, score: -510.96801818055826\n",
      "episode: 667/5000, score: -370.1611653261709\n",
      "episode: 668/5000, score: -67.48457451160886\n",
      "episode: 669/5000, score: -252.79049546591082\n",
      "episode: 670/5000, score: -291.4679633262663\n",
      "episode: 671/5000, score: -401.81504675351795\n",
      "episode: 672/5000, score: -266.4379182046699\n",
      "episode: 673/5000, score: -264.38529676969654\n",
      "episode: 674/5000, score: -161.0386789089441\n",
      "episode: 675/5000, score: -231.57624910020402\n",
      "episode: 676/5000, score: -387.0732157814895\n",
      "episode: 677/5000, score: -157.5520649412172\n",
      "episode: 678/5000, score: -129.8111872585318\n",
      "episode: 679/5000, score: -317.23868417854027\n",
      "episode: 680/5000, score: -218.3956674352974\n",
      "episode: 681/5000, score: -392.75602109085213\n",
      "episode: 682/5000, score: -436.34215320843884\n",
      "episode: 683/5000, score: -205.30378433195955\n",
      "episode: 684/5000, score: -357.5115631350365\n",
      "episode: 685/5000, score: -345.2840457818885\n",
      "episode: 686/5000, score: -176.60239880769615\n",
      "episode: 687/5000, score: -265.07134290796785\n",
      "episode: 688/5000, score: -186.62902375563357\n",
      "episode: 689/5000, score: -295.67592342902594\n",
      "episode: 690/5000, score: -135.27020757035152\n",
      "episode: 691/5000, score: -424.91548637141665\n",
      "episode: 692/5000, score: -121.05065484059759\n",
      "episode: 693/5000, score: -349.47477753807465\n",
      "episode: 694/5000, score: -184.91940038856035\n",
      "episode: 695/5000, score: -448.3403862401096\n",
      "episode: 696/5000, score: -391.6889457900367\n",
      "episode: 697/5000, score: -184.88524574631327\n",
      "episode: 698/5000, score: -148.6297664408915\n",
      "episode: 699/5000, score: -101.15368866102486\n",
      "episode: 700/5000, score: -194.0192270858989\n",
      "episode: 701/5000, score: -271.69460671452964\n",
      "episode: 702/5000, score: -333.6208551479243\n",
      "episode: 703/5000, score: -310.1899385199914\n",
      "episode: 704/5000, score: -807.0977515770144\n",
      "episode: 705/5000, score: -177.7853756556405\n",
      "episode: 706/5000, score: -195.50460694031514\n",
      "episode: 707/5000, score: -324.776800919855\n",
      "episode: 708/5000, score: -311.3363893743041\n",
      "episode: 709/5000, score: -305.28835509140833\n",
      "episode: 710/5000, score: -265.21254469758765\n",
      "episode: 711/5000, score: -37.531648498995665\n",
      "episode: 712/5000, score: -36.07985172395557\n",
      "episode: 713/5000, score: -317.28643097287085\n",
      "episode: 714/5000, score: -213.60710917582\n",
      "episode: 715/5000, score: -352.0363625502043\n",
      "episode: 716/5000, score: -231.03554353483565\n",
      "episode: 717/5000, score: -385.38252645034476\n",
      "episode: 718/5000, score: -249.06407054548788\n",
      "episode: 719/5000, score: -371.6480274747598\n",
      "episode: 720/5000, score: -376.5838336972518\n",
      "episode: 721/5000, score: -249.13641581424417\n",
      "episode: 722/5000, score: -238.1037317472616\n",
      "episode: 723/5000, score: -355.4095325108732\n",
      "episode: 724/5000, score: -148.19051411888543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 725/5000, score: -381.84587752075504\n",
      "episode: 726/5000, score: -365.6155321748407\n",
      "episode: 727/5000, score: -212.3887930556865\n",
      "episode: 728/5000, score: -182.93091373543223\n",
      "episode: 729/5000, score: -169.26193910187868\n",
      "episode: 730/5000, score: -307.9056581184158\n",
      "episode: 731/5000, score: -556.7590298586622\n",
      "episode: 732/5000, score: -277.971771428906\n",
      "episode: 733/5000, score: -381.71442391629324\n",
      "episode: 734/5000, score: -282.4986344684015\n",
      "episode: 735/5000, score: -125.59613787476553\n",
      "episode: 736/5000, score: -37.441580330835706\n",
      "episode: 737/5000, score: -249.8251975250735\n",
      "episode: 738/5000, score: -330.62451246307637\n",
      "episode: 739/5000, score: -95.13247633215552\n",
      "episode: 740/5000, score: -248.80034077894808\n",
      "episode: 741/5000, score: -308.8446346081027\n",
      "episode: 742/5000, score: -396.29341827581806\n",
      "episode: 743/5000, score: -183.12306650142204\n",
      "episode: 744/5000, score: -353.54435863156846\n",
      "episode: 745/5000, score: -500.2094050869641\n",
      "episode: 746/5000, score: -392.80032509532333\n",
      "episode: 747/5000, score: 51.78107090935393\n",
      "episode: 748/5000, score: -222.68267467842804\n",
      "episode: 749/5000, score: -401.2803477880271\n",
      "episode: 750/5000, score: -140.15175863569678\n",
      "episode: 751/5000, score: -31.477328413926784\n",
      "episode: 752/5000, score: -321.6769381910329\n",
      "episode: 753/5000, score: -415.0770445730042\n",
      "episode: 754/5000, score: 68.82107098322977\n",
      "episode: 755/5000, score: -589.5094876914224\n",
      "episode: 756/5000, score: -258.8852677958504\n",
      "episode: 757/5000, score: -266.1698735882768\n",
      "episode: 758/5000, score: -303.45977397314056\n",
      "episode: 759/5000, score: -448.5743697392356\n",
      "episode: 760/5000, score: -462.5581178973839\n",
      "episode: 761/5000, score: -537.5541083711836\n",
      "episode: 762/5000, score: -473.7797350509003\n",
      "episode: 763/5000, score: -268.6344764763615\n",
      "episode: 764/5000, score: -382.1638459426502\n",
      "episode: 765/5000, score: -23.59055273131726\n",
      "episode: 766/5000, score: -64.8436651870621\n",
      "episode: 767/5000, score: -21.523056239665166\n",
      "episode: 768/5000, score: -220.75566967787398\n",
      "episode: 769/5000, score: -168.6270063672627\n",
      "episode: 770/5000, score: -332.23313536742\n",
      "episode: 771/5000, score: -203.18420259247492\n",
      "episode: 772/5000, score: -208.14418829032599\n",
      "episode: 773/5000, score: -85.22558479221605\n",
      "episode: 774/5000, score: -207.50299070543187\n",
      "episode: 775/5000, score: -173.82698804469265\n",
      "episode: 776/5000, score: -0.2473748486733598\n",
      "episode: 777/5000, score: -320.7034992156106\n",
      "episode: 778/5000, score: -260.659253542433\n",
      "episode: 779/5000, score: -169.8757572544136\n",
      "episode: 780/5000, score: -269.62626612962333\n",
      "episode: 781/5000, score: -403.0049842628899\n",
      "episode: 782/5000, score: -227.6104280284335\n",
      "episode: 783/5000, score: -217.42313198674694\n",
      "episode: 784/5000, score: -331.4488071792029\n",
      "episode: 785/5000, score: -405.88512098785486\n",
      "episode: 786/5000, score: -437.4870188873027\n",
      "episode: 787/5000, score: -310.93147069763313\n",
      "episode: 788/5000, score: -275.15137299685557\n",
      "episode: 789/5000, score: -231.45169517329361\n",
      "episode: 790/5000, score: -222.68014650908344\n",
      "episode: 791/5000, score: -222.0107649861969\n",
      "episode: 792/5000, score: -177.21767738011454\n",
      "episode: 793/5000, score: -164.4392044030771\n",
      "episode: 794/5000, score: -323.63242275045894\n",
      "episode: 795/5000, score: -15.623967956187698\n",
      "episode: 796/5000, score: -53.70540008468204\n",
      "episode: 797/5000, score: -144.16275436710265\n",
      "episode: 798/5000, score: -279.89495640156736\n",
      "episode: 799/5000, score: -189.03280202107067\n",
      "episode: 800/5000, score: -320.3960475234551\n",
      "episode: 801/5000, score: -530.8387856095385\n",
      "episode: 802/5000, score: -383.51145540785274\n",
      "episode: 803/5000, score: -416.864093806956\n",
      "episode: 804/5000, score: -200.85505086766818\n",
      "episode: 805/5000, score: -207.73722098260401\n",
      "episode: 806/5000, score: -461.46613287178025\n",
      "episode: 807/5000, score: -219.67452903567033\n",
      "episode: 808/5000, score: -364.2237397595901\n",
      "episode: 809/5000, score: -149.63969725598952\n",
      "episode: 810/5000, score: -380.11359733284075\n",
      "episode: 811/5000, score: -198.80109116881502\n",
      "episode: 812/5000, score: -370.72830636844486\n",
      "episode: 813/5000, score: -414.8374721120026\n",
      "episode: 814/5000, score: -107.38733414133988\n",
      "episode: 815/5000, score: -444.3794069726769\n",
      "episode: 816/5000, score: -365.2151393691548\n",
      "episode: 817/5000, score: -369.806872496977\n",
      "episode: 818/5000, score: -232.97719789289386\n",
      "episode: 819/5000, score: -278.62690348943215\n",
      "episode: 820/5000, score: -223.2098741456176\n",
      "episode: 821/5000, score: -155.12765298741365\n",
      "episode: 822/5000, score: -244.73039558035325\n",
      "episode: 823/5000, score: -230.1963159347247\n",
      "episode: 824/5000, score: -315.36142237575507\n",
      "episode: 825/5000, score: -205.93186933282453\n",
      "episode: 826/5000, score: -148.55628825878722\n",
      "episode: 827/5000, score: -50.43644884168234\n",
      "episode: 828/5000, score: -271.05965705933386\n",
      "episode: 829/5000, score: -172.7839173039775\n",
      "episode: 830/5000, score: -325.6174526447732\n",
      "episode: 831/5000, score: -89.67675904945622\n",
      "episode: 832/5000, score: -138.87755408812484\n",
      "episode: 833/5000, score: -307.79031019502423\n",
      "episode: 834/5000, score: -315.7574415978654\n",
      "episode: 835/5000, score: -399.1309573411828\n",
      "episode: 836/5000, score: -169.1540380332408\n",
      "episode: 837/5000, score: -256.0433669304551\n",
      "episode: 838/5000, score: -157.11038797962587\n",
      "episode: 839/5000, score: -289.7925976755918\n",
      "episode: 840/5000, score: -156.45646949959848\n",
      "episode: 841/5000, score: -442.93172950791075\n",
      "episode: 842/5000, score: -491.53932015593216\n",
      "episode: 843/5000, score: -226.89023421561131\n",
      "episode: 844/5000, score: -587.5067643927939\n",
      "episode: 845/5000, score: -514.048547307349\n",
      "episode: 846/5000, score: -160.8108905167196\n",
      "episode: 847/5000, score: -376.87941761931245\n",
      "episode: 848/5000, score: -260.742548903302\n",
      "episode: 849/5000, score: -215.09697730013494\n",
      "episode: 850/5000, score: -321.06829033279564\n",
      "episode: 851/5000, score: -276.50428580685593\n",
      "episode: 852/5000, score: -168.93142694835802\n",
      "episode: 853/5000, score: -206.92129219264513\n",
      "episode: 854/5000, score: -225.8200246418104\n",
      "episode: 855/5000, score: -297.7407497923108\n",
      "episode: 856/5000, score: -240.5047387566458\n",
      "episode: 857/5000, score: -252.1930452701714\n",
      "episode: 858/5000, score: -221.53416344732113\n",
      "episode: 859/5000, score: -320.837872684001\n",
      "episode: 860/5000, score: -238.36223077475967\n",
      "episode: 861/5000, score: -293.3632579850656\n",
      "episode: 862/5000, score: -292.4187607676316\n",
      "episode: 863/5000, score: -148.2289377360728\n",
      "episode: 864/5000, score: -133.62015426535072\n",
      "episode: 865/5000, score: -144.6390876068072\n",
      "episode: 866/5000, score: -57.698007304590796\n",
      "episode: 867/5000, score: -277.75340552736077\n",
      "episode: 868/5000, score: -306.3000209082203\n",
      "episode: 869/5000, score: -155.75588628186955\n",
      "episode: 870/5000, score: -489.7722815966115\n",
      "episode: 871/5000, score: -139.14810870979113\n",
      "episode: 872/5000, score: -309.11536414563886\n",
      "episode: 873/5000, score: -114.96666899061118\n",
      "episode: 874/5000, score: -242.29806037903998\n",
      "episode: 875/5000, score: -152.23026902919057\n",
      "episode: 876/5000, score: -244.5842261495874\n",
      "episode: 877/5000, score: -362.0669588232367\n",
      "episode: 878/5000, score: 13.622502910088372\n",
      "episode: 879/5000, score: -86.43986384094131\n",
      "episode: 880/5000, score: -262.37186961043494\n",
      "episode: 881/5000, score: -289.4930110135424\n",
      "episode: 882/5000, score: -72.04450964133437\n",
      "episode: 883/5000, score: -257.0541681613089\n",
      "episode: 884/5000, score: -396.18828141549517\n",
      "episode: 885/5000, score: -226.57041785272082\n",
      "episode: 886/5000, score: -218.7338625232837\n",
      "episode: 887/5000, score: -154.60102759214766\n",
      "episode: 888/5000, score: -209.14817573486093\n",
      "episode: 889/5000, score: -334.25303375282994\n",
      "episode: 890/5000, score: -8.397617715830663\n",
      "episode: 891/5000, score: -274.39947160734994\n",
      "episode: 892/5000, score: -273.6451663918652\n",
      "episode: 893/5000, score: -218.22936994599237\n",
      "episode: 894/5000, score: -281.11931400675564\n",
      "episode: 895/5000, score: -247.74414216137063\n",
      "episode: 896/5000, score: -206.2063770305457\n",
      "episode: 897/5000, score: -432.4749952236567\n",
      "episode: 898/5000, score: -423.94238095166577\n",
      "episode: 899/5000, score: -215.2497269476466\n",
      "episode: 900/5000, score: -195.3970997068559\n",
      "episode: 901/5000, score: -122.02966006054176\n",
      "episode: 902/5000, score: -112.66436476692631\n",
      "episode: 903/5000, score: -449.3964484984009\n",
      "episode: 904/5000, score: -716.7224838172638\n",
      "episode: 905/5000, score: -281.33080374853677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 906/5000, score: -424.5823333896395\n",
      "episode: 907/5000, score: -454.40275342910354\n",
      "episode: 908/5000, score: -311.22088181322954\n",
      "episode: 909/5000, score: 11.992244912889078\n",
      "episode: 910/5000, score: -425.36192072530207\n",
      "episode: 911/5000, score: -535.3718758904206\n",
      "episode: 912/5000, score: -345.77304148741655\n",
      "episode: 913/5000, score: -253.8946793555087\n",
      "episode: 914/5000, score: -300.601594501259\n",
      "episode: 915/5000, score: -483.1118336576998\n",
      "episode: 916/5000, score: -235.60830431302858\n",
      "episode: 917/5000, score: -500.9688046294278\n",
      "episode: 918/5000, score: -361.86446884581596\n",
      "episode: 919/5000, score: -447.91469338737267\n",
      "episode: 920/5000, score: -621.1627219023959\n",
      "episode: 921/5000, score: -57.34348317708446\n",
      "episode: 922/5000, score: -152.7169433225344\n",
      "episode: 923/5000, score: -237.28621567428587\n",
      "episode: 924/5000, score: 30.28891012295395\n",
      "episode: 925/5000, score: -220.18568664162973\n",
      "episode: 926/5000, score: -75.74385830466305\n",
      "episode: 927/5000, score: -151.82366714916273\n",
      "episode: 928/5000, score: -265.57352544366734\n",
      "episode: 929/5000, score: -628.9006077107462\n",
      "episode: 930/5000, score: -492.4329176600411\n",
      "episode: 931/5000, score: -191.7775921971824\n",
      "episode: 932/5000, score: -111.1520025870714\n",
      "episode: 933/5000, score: -166.1585326191035\n",
      "episode: 934/5000, score: -362.7893674889549\n",
      "episode: 935/5000, score: -503.3726126057164\n",
      "episode: 936/5000, score: -136.78937481740377\n",
      "episode: 937/5000, score: -233.25349638917348\n",
      "episode: 938/5000, score: -113.9854310224497\n",
      "episode: 939/5000, score: -247.2636763650701\n",
      "episode: 940/5000, score: -222.25841977936733\n",
      "episode: 941/5000, score: -147.04473815774256\n",
      "episode: 942/5000, score: -246.52252874174025\n",
      "episode: 943/5000, score: -243.49870947695803\n",
      "episode: 944/5000, score: -58.96709680846599\n",
      "episode: 945/5000, score: -138.98384241446337\n",
      "episode: 946/5000, score: -137.25170958890138\n",
      "episode: 947/5000, score: -184.75398171968277\n",
      "episode: 948/5000, score: -319.71411129483033\n",
      "episode: 949/5000, score: -196.40650161068908\n",
      "episode: 950/5000, score: -464.0117321920543\n",
      "episode: 951/5000, score: -345.6717984990421\n",
      "episode: 952/5000, score: -312.61528336676196\n",
      "episode: 953/5000, score: -310.9360779253325\n",
      "episode: 954/5000, score: -544.7358606293283\n",
      "episode: 955/5000, score: -228.45013823998906\n",
      "episode: 956/5000, score: -233.59676321196233\n",
      "episode: 957/5000, score: -246.3207775767333\n",
      "episode: 958/5000, score: -120.22946655374649\n",
      "episode: 959/5000, score: -212.69533352958973\n",
      "episode: 960/5000, score: -269.86416680688166\n",
      "episode: 961/5000, score: -229.02087826486667\n",
      "episode: 962/5000, score: -267.26153520043806\n",
      "episode: 963/5000, score: -268.7869170910841\n",
      "episode: 964/5000, score: -105.65991972095176\n",
      "episode: 965/5000, score: -88.22171479269463\n",
      "episode: 966/5000, score: -306.1366971454063\n",
      "episode: 967/5000, score: -544.2272195843802\n",
      "episode: 968/5000, score: -461.37587600922666\n",
      "episode: 969/5000, score: -334.803893034506\n",
      "episode: 970/5000, score: -397.74057753590654\n",
      "episode: 971/5000, score: -219.46577977314266\n",
      "episode: 972/5000, score: -195.34486389006898\n",
      "episode: 973/5000, score: -232.16327310488037\n",
      "episode: 974/5000, score: -480.9562358588744\n",
      "episode: 975/5000, score: -343.5939156409602\n",
      "episode: 976/5000, score: -150.33371708518354\n",
      "episode: 977/5000, score: -472.17120953124476\n",
      "episode: 978/5000, score: -406.5015826796006\n",
      "episode: 979/5000, score: -235.2878765798556\n",
      "episode: 980/5000, score: -220.0763844840984\n",
      "episode: 981/5000, score: -194.4392232653995\n",
      "episode: 982/5000, score: -52.21973978261131\n",
      "episode: 983/5000, score: -469.7254798131293\n",
      "episode: 984/5000, score: -483.8927731868622\n",
      "episode: 985/5000, score: -288.53052614345927\n",
      "episode: 986/5000, score: -272.4125494395697\n",
      "episode: 987/5000, score: -172.36471234759046\n",
      "episode: 988/5000, score: -254.094451938488\n",
      "episode: 989/5000, score: -399.3709437700091\n",
      "episode: 990/5000, score: 42.304153422894785\n",
      "episode: 991/5000, score: -227.25421792456672\n",
      "episode: 992/5000, score: -381.32335760360996\n",
      "episode: 993/5000, score: -336.44746450297146\n",
      "episode: 994/5000, score: -218.7002470911311\n",
      "episode: 995/5000, score: -242.0561413687536\n",
      "episode: 996/5000, score: -415.4235508030637\n",
      "episode: 997/5000, score: -266.4008410396441\n",
      "episode: 998/5000, score: -309.8783096381338\n",
      "episode: 999/5000, score: -256.0861980971471\n",
      "episode: 1000/5000, score: -368.7216564173062\n",
      "episode: 1001/5000, score: -330.1085396633739\n",
      "episode: 1002/5000, score: -379.75463032797256\n",
      "episode: 1003/5000, score: -210.96837299673987\n",
      "episode: 1004/5000, score: -306.1599873914947\n",
      "episode: 1005/5000, score: -344.4114685571665\n",
      "episode: 1006/5000, score: -277.32669834420483\n",
      "episode: 1007/5000, score: -269.35697328462027\n",
      "episode: 1008/5000, score: -298.6501595311985\n",
      "episode: 1009/5000, score: -164.11380158430165\n",
      "episode: 1010/5000, score: -147.31325560991456\n",
      "episode: 1011/5000, score: -416.3945640072385\n",
      "episode: 1012/5000, score: -450.34582743237485\n",
      "episode: 1013/5000, score: -179.8968831361206\n",
      "episode: 1014/5000, score: -396.00765655927955\n",
      "episode: 1015/5000, score: -159.1479796325774\n",
      "episode: 1016/5000, score: -168.74888427687236\n",
      "episode: 1017/5000, score: -231.70757969263968\n",
      "episode: 1018/5000, score: -358.7107483986369\n",
      "episode: 1019/5000, score: -403.887404183266\n",
      "episode: 1020/5000, score: -428.9399813564648\n",
      "episode: 1021/5000, score: -456.89043955794324\n",
      "episode: 1022/5000, score: -59.20895274119957\n",
      "episode: 1023/5000, score: -148.2005380418301\n",
      "episode: 1024/5000, score: -300.42086411444313\n",
      "episode: 1025/5000, score: -394.0239108363919\n",
      "episode: 1026/5000, score: -405.45082078962884\n",
      "episode: 1027/5000, score: -524.3968595453058\n",
      "episode: 1028/5000, score: -391.35073997923894\n",
      "episode: 1029/5000, score: -348.14593463636857\n",
      "episode: 1030/5000, score: -295.0693354843188\n",
      "episode: 1031/5000, score: -449.8887488967817\n",
      "episode: 1032/5000, score: -243.87317449542363\n",
      "episode: 1033/5000, score: -156.67606242709294\n",
      "episode: 1034/5000, score: -242.59122844296826\n",
      "episode: 1035/5000, score: -353.3617418297559\n",
      "episode: 1036/5000, score: -495.19203646758166\n",
      "episode: 1037/5000, score: -342.1832770939916\n",
      "episode: 1038/5000, score: -393.6796870005952\n",
      "episode: 1039/5000, score: -136.2576323796485\n",
      "episode: 1040/5000, score: -84.75261915210989\n",
      "episode: 1041/5000, score: -310.182335385553\n",
      "episode: 1042/5000, score: -300.80609752223734\n",
      "episode: 1043/5000, score: -341.13080083410006\n",
      "episode: 1044/5000, score: -423.67690856069333\n",
      "episode: 1045/5000, score: -119.40209565131211\n",
      "episode: 1046/5000, score: -316.18753670304875\n",
      "episode: 1047/5000, score: -376.0192181406696\n",
      "episode: 1048/5000, score: -456.55418098712227\n",
      "episode: 1049/5000, score: -249.96162728327886\n",
      "episode: 1050/5000, score: -242.2456825679805\n",
      "episode: 1051/5000, score: -245.04934167427587\n",
      "episode: 1052/5000, score: -129.163810416978\n",
      "episode: 1053/5000, score: -323.61912473575967\n",
      "episode: 1054/5000, score: -388.591830909125\n",
      "episode: 1055/5000, score: -433.49074855440267\n",
      "episode: 1056/5000, score: -185.59610122163292\n",
      "episode: 1057/5000, score: -399.482073989241\n",
      "episode: 1058/5000, score: -398.0093034280557\n",
      "episode: 1059/5000, score: -401.3644510465602\n",
      "episode: 1060/5000, score: -130.93003247397615\n",
      "episode: 1061/5000, score: -367.8671901708325\n",
      "episode: 1062/5000, score: -369.8861147976895\n",
      "episode: 1063/5000, score: -147.82369697310648\n",
      "episode: 1064/5000, score: -148.3529094154075\n",
      "episode: 1065/5000, score: -445.3221541564334\n",
      "episode: 1066/5000, score: -398.2149189981199\n",
      "episode: 1067/5000, score: -206.02027389310064\n",
      "episode: 1068/5000, score: -83.0491368057681\n",
      "episode: 1069/5000, score: -495.76470680373575\n",
      "episode: 1070/5000, score: -123.74405538056303\n",
      "episode: 1071/5000, score: -128.95072565116124\n",
      "episode: 1072/5000, score: -354.95479854544095\n",
      "episode: 1073/5000, score: -442.0433761514682\n",
      "episode: 1074/5000, score: -370.50273799444807\n",
      "episode: 1075/5000, score: -137.3279031990211\n",
      "episode: 1076/5000, score: -353.8154630693398\n",
      "episode: 1077/5000, score: -326.67079701366094\n",
      "episode: 1078/5000, score: -501.081880904107\n",
      "episode: 1079/5000, score: -245.9188968821427\n",
      "episode: 1080/5000, score: -135.69354643598226\n",
      "episode: 1081/5000, score: -428.44709216749993\n",
      "episode: 1082/5000, score: -400.67789572092687\n",
      "episode: 1083/5000, score: -97.70615569947242\n",
      "episode: 1084/5000, score: -126.8509070361971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1085/5000, score: -205.10216427598974\n",
      "episode: 1086/5000, score: -300.5902517265795\n",
      "episode: 1087/5000, score: -360.8319908793471\n",
      "episode: 1088/5000, score: -179.6991373652637\n",
      "episode: 1089/5000, score: -152.26179580756883\n",
      "episode: 1090/5000, score: -358.9988030384427\n",
      "episode: 1091/5000, score: -78.50150596068151\n",
      "episode: 1092/5000, score: -389.09468474180676\n",
      "episode: 1093/5000, score: -422.2875221201664\n",
      "episode: 1094/5000, score: -447.97250996036274\n",
      "episode: 1095/5000, score: -394.8531246865094\n",
      "episode: 1096/5000, score: -474.5643714767552\n",
      "episode: 1097/5000, score: -400.00205620922594\n",
      "episode: 1098/5000, score: -464.62464937458003\n",
      "episode: 1099/5000, score: -440.6826051041051\n",
      "episode: 1100/5000, score: -378.76668475046125\n",
      "episode: 1101/5000, score: -459.3206948013461\n",
      "episode: 1102/5000, score: -355.9027451532726\n",
      "episode: 1103/5000, score: -416.5244421361186\n",
      "episode: 1104/5000, score: -419.5598116161777\n",
      "episode: 1105/5000, score: -428.64439787135274\n",
      "episode: 1106/5000, score: -118.77258769302553\n",
      "episode: 1107/5000, score: -402.56819752953436\n",
      "episode: 1108/5000, score: -385.22390134718756\n",
      "episode: 1109/5000, score: -439.70691613423253\n",
      "episode: 1110/5000, score: -402.0411863193432\n",
      "episode: 1111/5000, score: -417.3747296570838\n",
      "episode: 1112/5000, score: -416.5570317637282\n",
      "episode: 1113/5000, score: -312.9534327383593\n",
      "episode: 1114/5000, score: -419.55187677071905\n",
      "episode: 1115/5000, score: -383.7451459003362\n",
      "episode: 1116/5000, score: -386.81554361324083\n",
      "episode: 1117/5000, score: -464.87774224089\n",
      "episode: 1118/5000, score: -144.40472094950496\n",
      "episode: 1119/5000, score: -414.1437968795886\n",
      "episode: 1120/5000, score: -393.4921463624645\n",
      "episode: 1121/5000, score: -213.727898896335\n",
      "episode: 1122/5000, score: -375.3982400196247\n",
      "episode: 1123/5000, score: -121.74728680672901\n",
      "episode: 1124/5000, score: -125.57187279277957\n",
      "episode: 1125/5000, score: -121.47052168854528\n",
      "episode: 1126/5000, score: -124.45575869717234\n",
      "episode: 1127/5000, score: -128.08773677558568\n",
      "episode: 1128/5000, score: -154.6769653992559\n",
      "episode: 1129/5000, score: -390.2754162649069\n",
      "episode: 1130/5000, score: -424.99229471448683\n",
      "episode: 1131/5000, score: -8.12414750882428\n",
      "episode: 1132/5000, score: -326.2480139862723\n",
      "episode: 1133/5000, score: -376.10547882450464\n",
      "episode: 1134/5000, score: -143.5815792027206\n",
      "episode: 1135/5000, score: -128.38900093434384\n",
      "episode: 1136/5000, score: -212.3756334991761\n",
      "episode: 1137/5000, score: -121.01147324480587\n",
      "episode: 1138/5000, score: -320.3401446323503\n",
      "episode: 1139/5000, score: -71.23354990662662\n",
      "episode: 1140/5000, score: -189.93976034555072\n",
      "episode: 1141/5000, score: -229.6765570185185\n",
      "episode: 1142/5000, score: -8.020427720633649\n",
      "episode: 1143/5000, score: -148.82466984905312\n",
      "episode: 1144/5000, score: -296.37850627306307\n",
      "episode: 1145/5000, score: -363.91862938992995\n",
      "episode: 1146/5000, score: -195.37431964565656\n",
      "episode: 1147/5000, score: -343.24879757890557\n",
      "episode: 1148/5000, score: -110.21271711085274\n",
      "episode: 1149/5000, score: -109.4090731040201\n",
      "episode: 1150/5000, score: -106.63952910015789\n",
      "episode: 1151/5000, score: -207.66805348544727\n",
      "episode: 1152/5000, score: -367.22615967655065\n",
      "episode: 1153/5000, score: -196.45945508877838\n",
      "episode: 1154/5000, score: -355.38375308557715\n",
      "episode: 1155/5000, score: -403.3870723167163\n",
      "episode: 1156/5000, score: -103.48793689216886\n",
      "episode: 1157/5000, score: -271.39287982336566\n",
      "episode: 1158/5000, score: -455.367984349643\n",
      "episode: 1159/5000, score: -108.80323924005089\n",
      "episode: 1160/5000, score: -373.9652801170098\n",
      "episode: 1161/5000, score: -176.6815613989175\n",
      "episode: 1162/5000, score: -484.027706855286\n",
      "episode: 1163/5000, score: -164.1511291367537\n",
      "episode: 1164/5000, score: -138.5183220863499\n",
      "episode: 1165/5000, score: -143.7907392123009\n",
      "episode: 1166/5000, score: -236.4885567943073\n",
      "episode: 1167/5000, score: -345.02385410792715\n",
      "episode: 1168/5000, score: -132.62303195540858\n",
      "episode: 1169/5000, score: -72.68902897177145\n",
      "episode: 1170/5000, score: -121.90452519933115\n",
      "episode: 1171/5000, score: -119.63608292760911\n",
      "episode: 1172/5000, score: -154.86097239802376\n",
      "episode: 1173/5000, score: -12.220246707312441\n",
      "episode: 1174/5000, score: -141.10399822196447\n",
      "episode: 1175/5000, score: -356.29047328607925\n",
      "episode: 1176/5000, score: -416.9788242796407\n",
      "episode: 1177/5000, score: -370.3916057040102\n",
      "episode: 1178/5000, score: -362.496581992945\n",
      "episode: 1179/5000, score: -455.1229527357422\n",
      "episode: 1180/5000, score: -110.99609403829031\n",
      "episode: 1181/5000, score: -256.80105154454924\n",
      "episode: 1182/5000, score: -379.7901523957015\n",
      "episode: 1183/5000, score: -117.98463407409659\n",
      "episode: 1184/5000, score: -199.44891128599954\n",
      "episode: 1185/5000, score: -149.66000261275283\n",
      "episode: 1186/5000, score: -194.7771744924488\n",
      "episode: 1187/5000, score: -101.31439354098356\n",
      "episode: 1188/5000, score: -143.32359895458717\n",
      "episode: 1189/5000, score: -164.983448757212\n",
      "episode: 1190/5000, score: -228.79849818698332\n",
      "episode: 1191/5000, score: -133.97989804415533\n",
      "episode: 1192/5000, score: -163.33261650021615\n",
      "episode: 1193/5000, score: -301.4284026546271\n",
      "episode: 1194/5000, score: -356.85455409223664\n",
      "episode: 1195/5000, score: -264.782305178725\n",
      "episode: 1196/5000, score: -298.95687446214265\n",
      "episode: 1197/5000, score: -232.18913776863386\n",
      "episode: 1198/5000, score: -57.46341050850063\n",
      "episode: 1199/5000, score: -112.57522733421818\n",
      "episode: 1200/5000, score: -193.6581072964267\n",
      "episode: 1201/5000, score: -72.42712219397023\n",
      "episode: 1202/5000, score: -107.33917795576173\n",
      "episode: 1203/5000, score: -205.45767298109357\n",
      "episode: 1204/5000, score: -194.45070089789502\n",
      "episode: 1205/5000, score: -63.273741131209974\n",
      "episode: 1206/5000, score: -60.41198734820631\n",
      "episode: 1207/5000, score: -229.04602198558555\n",
      "episode: 1208/5000, score: -134.61516439368842\n",
      "episode: 1209/5000, score: -211.25048320175915\n",
      "episode: 1210/5000, score: -114.88896147792619\n",
      "episode: 1211/5000, score: -104.82109550222418\n",
      "episode: 1212/5000, score: -125.12389296506726\n",
      "episode: 1213/5000, score: -350.68769225457294\n",
      "episode: 1214/5000, score: -273.06540061055637\n",
      "episode: 1215/5000, score: -419.4827313975607\n",
      "episode: 1216/5000, score: -325.715788940745\n",
      "episode: 1217/5000, score: -120.58961458861461\n",
      "episode: 1218/5000, score: -240.68862572001365\n",
      "episode: 1219/5000, score: -26.71890339900049\n",
      "episode: 1220/5000, score: -172.51536681732716\n",
      "episode: 1221/5000, score: -302.13387776595044\n",
      "episode: 1222/5000, score: -303.60238281276224\n",
      "episode: 1223/5000, score: -115.40558623076922\n",
      "episode: 1224/5000, score: -403.94611238702515\n",
      "episode: 1225/5000, score: -334.55394400846285\n",
      "episode: 1226/5000, score: -122.8770686512623\n",
      "episode: 1227/5000, score: -346.5900402246422\n",
      "episode: 1228/5000, score: -394.26323710773585\n",
      "episode: 1229/5000, score: -226.8996886907488\n",
      "episode: 1230/5000, score: -476.7737056280847\n",
      "episode: 1231/5000, score: -35.31820532325236\n",
      "episode: 1232/5000, score: -280.623042447326\n",
      "episode: 1233/5000, score: -188.4577044970775\n",
      "episode: 1234/5000, score: -87.64524757175431\n",
      "episode: 1235/5000, score: -327.4351845159308\n",
      "episode: 1236/5000, score: -295.97495362679524\n",
      "episode: 1237/5000, score: -421.54630959825874\n",
      "episode: 1238/5000, score: -156.51643579320594\n",
      "episode: 1239/5000, score: -335.72250008348\n",
      "episode: 1240/5000, score: -304.10569884532975\n",
      "episode: 1241/5000, score: -351.3739214384266\n",
      "episode: 1242/5000, score: -168.5580325632494\n",
      "episode: 1243/5000, score: -219.19851095297923\n",
      "episode: 1244/5000, score: -270.30721381968175\n",
      "episode: 1245/5000, score: -257.33836881368495\n",
      "episode: 1246/5000, score: -192.46051328737167\n",
      "episode: 1247/5000, score: -177.64282004933733\n",
      "episode: 1248/5000, score: -377.58484577673187\n",
      "episode: 1249/5000, score: 239.94809732889436\n",
      "episode: 1250/5000, score: -236.87004687473367\n",
      "episode: 1251/5000, score: -340.0442146297316\n",
      "episode: 1252/5000, score: -135.73570182703355\n",
      "episode: 1253/5000, score: -82.10906516106363\n",
      "episode: 1254/5000, score: -147.061201026979\n",
      "episode: 1255/5000, score: -253.53464726992274\n",
      "episode: 1256/5000, score: -225.01330339924976\n",
      "episode: 1257/5000, score: -98.97888316271282\n",
      "episode: 1258/5000, score: -203.44360149022714\n",
      "episode: 1259/5000, score: -320.0839231536398\n",
      "episode: 1260/5000, score: -42.22476999375715\n",
      "episode: 1261/5000, score: -310.43209285004974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1262/5000, score: -182.57842641681864\n",
      "episode: 1263/5000, score: -231.19125811864274\n",
      "episode: 1264/5000, score: -539.9101529329756\n",
      "episode: 1265/5000, score: -282.2666385011238\n",
      "episode: 1266/5000, score: -289.8926314755585\n",
      "episode: 1267/5000, score: -42.89350709128806\n",
      "episode: 1268/5000, score: -252.53145571296218\n",
      "episode: 1269/5000, score: -13.897698911269643\n",
      "episode: 1270/5000, score: -270.09236156113656\n",
      "episode: 1271/5000, score: -292.81962486929\n",
      "episode: 1272/5000, score: -168.0147783606914\n",
      "episode: 1273/5000, score: -94.42005847230112\n",
      "episode: 1274/5000, score: -211.2519927989751\n",
      "episode: 1275/5000, score: -102.16734368460193\n",
      "episode: 1276/5000, score: -522.7324638438224\n",
      "episode: 1277/5000, score: -220.93101886750512\n",
      "episode: 1278/5000, score: -205.48868002231325\n",
      "episode: 1279/5000, score: -230.93947849583333\n",
      "episode: 1280/5000, score: -194.62772144214668\n",
      "episode: 1281/5000, score: -175.15412331521043\n",
      "episode: 1282/5000, score: -229.32310526641\n",
      "episode: 1283/5000, score: -211.19152003886785\n",
      "episode: 1284/5000, score: -267.3702603573879\n",
      "episode: 1285/5000, score: -313.9817085245518\n",
      "episode: 1286/5000, score: -315.57318034793104\n",
      "episode: 1287/5000, score: -106.63297079687096\n",
      "episode: 1288/5000, score: -261.6182742974812\n",
      "episode: 1289/5000, score: -161.17423187929361\n",
      "episode: 1290/5000, score: -159.15972765814166\n",
      "episode: 1291/5000, score: -290.08923561360496\n",
      "episode: 1292/5000, score: -231.38375378222568\n",
      "episode: 1293/5000, score: -254.03777303619785\n",
      "episode: 1294/5000, score: -306.61602919869836\n",
      "episode: 1295/5000, score: -406.6121222928767\n",
      "episode: 1296/5000, score: -295.94579024248986\n",
      "episode: 1297/5000, score: -219.6374258649637\n",
      "episode: 1298/5000, score: -562.298012402716\n",
      "episode: 1299/5000, score: -254.48132682895215\n",
      "episode: 1300/5000, score: -292.28927034313074\n",
      "episode: 1301/5000, score: -193.68411150928011\n",
      "episode: 1302/5000, score: -478.67929139633173\n",
      "episode: 1303/5000, score: -196.34087338330534\n",
      "episode: 1304/5000, score: -312.0878617347734\n",
      "episode: 1305/5000, score: -434.3264424289406\n",
      "episode: 1306/5000, score: -343.9239756460425\n",
      "episode: 1307/5000, score: -415.08545925432173\n",
      "episode: 1308/5000, score: -284.8042970969317\n",
      "episode: 1309/5000, score: -210.1811204218938\n",
      "episode: 1310/5000, score: -155.14876921031828\n",
      "episode: 1311/5000, score: -343.80517513283587\n",
      "episode: 1312/5000, score: -490.0247678743405\n",
      "episode: 1313/5000, score: -425.19615521140804\n",
      "episode: 1314/5000, score: -450.6837774617062\n",
      "episode: 1315/5000, score: -166.41325829343958\n",
      "episode: 1316/5000, score: -461.35285279564465\n",
      "episode: 1317/5000, score: -422.4746687225929\n",
      "episode: 1318/5000, score: -215.93937755997302\n",
      "episode: 1319/5000, score: -245.91064725856327\n",
      "episode: 1320/5000, score: -392.40811393138915\n",
      "episode: 1321/5000, score: -294.909865634541\n",
      "episode: 1322/5000, score: -156.04811892310468\n",
      "episode: 1323/5000, score: -426.8543443552582\n",
      "episode: 1324/5000, score: -448.5337645324559\n",
      "episode: 1325/5000, score: -398.7819869227253\n",
      "episode: 1326/5000, score: -389.896447941739\n",
      "episode: 1327/5000, score: -419.4314558921826\n",
      "episode: 1328/5000, score: -332.59801629180276\n",
      "episode: 1329/5000, score: -392.97726690702564\n",
      "episode: 1330/5000, score: 221.241482779143\n",
      "episode: 1331/5000, score: -494.97661085976273\n",
      "episode: 1332/5000, score: -445.81292604763337\n",
      "episode: 1333/5000, score: -188.22121445979005\n",
      "episode: 1334/5000, score: -403.1620289188424\n",
      "episode: 1335/5000, score: -280.18477449052057\n",
      "episode: 1336/5000, score: -120.14966803820029\n",
      "episode: 1337/5000, score: -303.1158221107571\n",
      "episode: 1338/5000, score: -395.3364861226818\n",
      "episode: 1339/5000, score: -177.26380191445656\n",
      "episode: 1340/5000, score: -177.1765494774003\n",
      "episode: 1341/5000, score: -128.47111515160316\n",
      "episode: 1342/5000, score: -288.5040171936139\n",
      "episode: 1343/5000, score: -107.94772546084161\n",
      "episode: 1344/5000, score: -431.7066454612685\n",
      "episode: 1345/5000, score: -293.1111493851364\n",
      "episode: 1346/5000, score: -168.48744281863677\n",
      "episode: 1347/5000, score: -77.29179250702803\n",
      "episode: 1348/5000, score: -165.4880362127272\n",
      "episode: 1349/5000, score: -100.81179836555825\n",
      "episode: 1350/5000, score: -154.0219344600725\n",
      "episode: 1351/5000, score: -237.29507520479973\n",
      "episode: 1352/5000, score: -115.18050414033115\n",
      "episode: 1353/5000, score: -223.07931347360085\n",
      "episode: 1354/5000, score: -212.95886571209127\n",
      "episode: 1355/5000, score: -154.23953091868734\n",
      "episode: 1356/5000, score: -199.23006697593564\n",
      "episode: 1357/5000, score: -193.18291976591374\n",
      "episode: 1358/5000, score: -123.11764417562193\n",
      "episode: 1359/5000, score: -201.98162353597712\n",
      "episode: 1360/5000, score: -30.86825110876933\n",
      "episode: 1361/5000, score: -280.28980400903305\n",
      "episode: 1362/5000, score: -180.6678277893238\n",
      "episode: 1363/5000, score: -167.6158956321089\n",
      "episode: 1364/5000, score: -157.6471726749222\n",
      "episode: 1365/5000, score: -172.33656974881248\n",
      "episode: 1366/5000, score: -161.0773234279702\n",
      "episode: 1367/5000, score: -160.56482357114294\n",
      "episode: 1368/5000, score: -180.93506147994822\n",
      "episode: 1369/5000, score: -195.5741395086115\n",
      "episode: 1370/5000, score: -168.16855929485993\n",
      "episode: 1371/5000, score: -155.7660294686914\n",
      "episode: 1372/5000, score: -195.7956725796996\n",
      "episode: 1373/5000, score: -213.90392664907515\n",
      "episode: 1374/5000, score: -221.54765658599996\n",
      "episode: 1375/5000, score: -232.81476329747613\n",
      "episode: 1376/5000, score: -240.97900151350933\n",
      "episode: 1377/5000, score: -247.5462023224708\n",
      "episode: 1378/5000, score: -232.80063697946804\n",
      "episode: 1379/5000, score: -188.03667275745363\n",
      "episode: 1380/5000, score: -232.31177500934942\n",
      "episode: 1381/5000, score: -234.017232129107\n",
      "episode: 1382/5000, score: -312.77871827867625\n",
      "episode: 1383/5000, score: -231.45298382182426\n",
      "episode: 1384/5000, score: -333.75587302388794\n",
      "episode: 1385/5000, score: -205.30765070295251\n",
      "episode: 1386/5000, score: -155.00739683662383\n",
      "episode: 1387/5000, score: -186.23546330721308\n",
      "episode: 1388/5000, score: -179.6025958546505\n",
      "episode: 1389/5000, score: -54.759054695072024\n",
      "episode: 1390/5000, score: -207.83586312415653\n",
      "episode: 1391/5000, score: -130.94172095575993\n",
      "episode: 1392/5000, score: -263.4101790929718\n",
      "episode: 1393/5000, score: -130.95399033023605\n",
      "episode: 1394/5000, score: -214.67697730437365\n",
      "episode: 1395/5000, score: -222.82322620121172\n",
      "episode: 1396/5000, score: -134.7570565965774\n",
      "episode: 1397/5000, score: -210.95093474712374\n",
      "episode: 1398/5000, score: -228.47767640668943\n",
      "episode: 1399/5000, score: -185.60426317659935\n",
      "episode: 1400/5000, score: -179.49304863656914\n",
      "episode: 1401/5000, score: -245.44614507146323\n",
      "episode: 1402/5000, score: -231.41956117965327\n",
      "episode: 1403/5000, score: -219.59876315025795\n",
      "episode: 1404/5000, score: -197.13339404071976\n",
      "episode: 1405/5000, score: -226.28934092279763\n",
      "episode: 1406/5000, score: -232.22357268635824\n",
      "episode: 1407/5000, score: -196.49927771623496\n",
      "episode: 1408/5000, score: -255.66726302794495\n",
      "episode: 1409/5000, score: -193.1727295211466\n",
      "episode: 1410/5000, score: -230.28718298956562\n",
      "episode: 1411/5000, score: -213.22886578810585\n",
      "episode: 1412/5000, score: -164.21684273230068\n",
      "episode: 1413/5000, score: -187.531391925101\n",
      "episode: 1414/5000, score: -116.28515525824845\n",
      "episode: 1415/5000, score: -197.92754909429615\n",
      "episode: 1416/5000, score: -160.95797660096702\n",
      "episode: 1417/5000, score: -142.7294282210182\n",
      "episode: 1418/5000, score: -152.19680833666703\n",
      "episode: 1419/5000, score: -29.196764042607082\n",
      "episode: 1420/5000, score: -165.0050491097123\n",
      "episode: 1421/5000, score: -168.12119831322062\n",
      "episode: 1422/5000, score: -203.89963336698818\n",
      "episode: 1423/5000, score: -304.7308010075339\n",
      "episode: 1424/5000, score: -171.07732276358217\n",
      "episode: 1425/5000, score: -245.42814984117635\n",
      "episode: 1426/5000, score: -75.6241328446986\n",
      "episode: 1427/5000, score: -157.79494243116275\n",
      "episode: 1428/5000, score: -136.3041145765085\n",
      "episode: 1429/5000, score: -117.79636003219177\n",
      "episode: 1430/5000, score: -129.60679062909804\n",
      "episode: 1431/5000, score: -131.7382808603755\n",
      "episode: 1432/5000, score: -99.77213950894611\n",
      "episode: 1433/5000, score: -51.901444410974825\n",
      "episode: 1434/5000, score: -147.66706497102948\n",
      "episode: 1435/5000, score: -125.75489471840245\n",
      "episode: 1436/5000, score: -123.27405463101732\n",
      "episode: 1437/5000, score: -136.68244079075652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1438/5000, score: -143.3311748300194\n",
      "episode: 1439/5000, score: -160.3189634355689\n",
      "episode: 1440/5000, score: -157.94138522326062\n",
      "episode: 1441/5000, score: -139.19942350928056\n",
      "episode: 1442/5000, score: -86.10915193499804\n",
      "episode: 1443/5000, score: -132.20450516110057\n",
      "episode: 1444/5000, score: -129.4119678093312\n",
      "episode: 1445/5000, score: -135.03157728160724\n",
      "episode: 1446/5000, score: -154.36862826508855\n",
      "episode: 1447/5000, score: -106.09800540949277\n",
      "episode: 1448/5000, score: -86.89588172153634\n",
      "episode: 1449/5000, score: -95.03683153182605\n",
      "episode: 1450/5000, score: -42.12651023696499\n",
      "episode: 1451/5000, score: 190.72566910800106\n",
      "episode: 1452/5000, score: -12.937931558777308\n",
      "episode: 1453/5000, score: -55.33945380177133\n",
      "episode: 1454/5000, score: -36.522941762829895\n",
      "episode: 1455/5000, score: -48.686589444434546\n",
      "episode: 1456/5000, score: 12.355596854658657\n",
      "episode: 1457/5000, score: -33.269787614310914\n",
      "episode: 1458/5000, score: -133.9959968724559\n",
      "episode: 1459/5000, score: -51.778885022963564\n",
      "episode: 1460/5000, score: -15.755834764641747\n",
      "episode: 1461/5000, score: -42.78901920156199\n",
      "episode: 1462/5000, score: -47.48192632860983\n",
      "episode: 1463/5000, score: -14.615940974143427\n",
      "episode: 1464/5000, score: -79.6856105598397\n",
      "episode: 1465/5000, score: -132.72075489959522\n",
      "episode: 1466/5000, score: -68.58910882635021\n",
      "episode: 1467/5000, score: 209.47339512956938\n",
      "episode: 1468/5000, score: -105.94919043348155\n",
      "episode: 1469/5000, score: -132.86667714273526\n",
      "episode: 1470/5000, score: -45.53411313383706\n",
      "episode: 1471/5000, score: -145.71611761016788\n",
      "episode: 1472/5000, score: -137.68238689109057\n",
      "episode: 1473/5000, score: 7.700476758998026\n",
      "episode: 1474/5000, score: -57.031716391600185\n",
      "episode: 1475/5000, score: -63.73243065867175\n",
      "episode: 1476/5000, score: -119.6337942746615\n",
      "episode: 1477/5000, score: -114.77319779420024\n",
      "episode: 1478/5000, score: -29.017575828828186\n",
      "episode: 1479/5000, score: -65.03658641445998\n",
      "episode: 1480/5000, score: -115.73779513725377\n",
      "episode: 1481/5000, score: -65.72407998451799\n",
      "episode: 1482/5000, score: -100.45432023102906\n",
      "episode: 1483/5000, score: -159.94493455946892\n",
      "episode: 1484/5000, score: -89.59791119267912\n",
      "episode: 1485/5000, score: -127.39021807158768\n",
      "episode: 1486/5000, score: -122.07119465786033\n",
      "episode: 1487/5000, score: -175.49754652256075\n",
      "episode: 1488/5000, score: -141.72986792806836\n",
      "episode: 1489/5000, score: -127.51663629875227\n",
      "episode: 1490/5000, score: -78.13982133582918\n",
      "episode: 1491/5000, score: -111.71668655471517\n",
      "episode: 1492/5000, score: -100.58242661175473\n",
      "episode: 1493/5000, score: -10.421897580468922\n",
      "episode: 1494/5000, score: -5.2707650647906945\n",
      "episode: 1495/5000, score: -244.06942532129148\n",
      "episode: 1496/5000, score: -40.66537483957591\n",
      "episode: 1497/5000, score: -182.28913176702872\n",
      "episode: 1498/5000, score: -162.37148557747204\n",
      "episode: 1499/5000, score: -111.10797657049783\n",
      "episode: 1500/5000, score: -86.93980577139935\n",
      "episode: 1501/5000, score: -28.42731839687231\n",
      "episode: 1502/5000, score: -141.12053216238044\n",
      "episode: 1503/5000, score: -162.02149954173075\n",
      "episode: 1504/5000, score: -94.31102184324344\n",
      "episode: 1505/5000, score: -22.646330837566268\n",
      "episode: 1506/5000, score: -69.63818386128294\n",
      "episode: 1507/5000, score: -50.23490131341119\n",
      "episode: 1508/5000, score: -73.90754674069098\n",
      "episode: 1509/5000, score: -61.03371598994975\n",
      "episode: 1510/5000, score: -117.68624082099825\n",
      "episode: 1511/5000, score: 240.04430950410003\n",
      "episode: 1512/5000, score: -145.01194587131934\n",
      "episode: 1513/5000, score: -183.97101245701168\n",
      "episode: 1514/5000, score: -122.8557804609783\n",
      "episode: 1515/5000, score: -101.84971845839989\n",
      "episode: 1516/5000, score: -86.67917713051071\n",
      "episode: 1517/5000, score: -57.42791814014122\n",
      "episode: 1518/5000, score: -164.17136064402513\n",
      "episode: 1519/5000, score: -142.11961952057916\n",
      "episode: 1520/5000, score: -206.81831828586695\n",
      "episode: 1521/5000, score: -134.96420989833743\n",
      "episode: 1522/5000, score: -161.67633827286494\n",
      "episode: 1523/5000, score: -197.13928026725603\n",
      "episode: 1524/5000, score: -195.64780923067576\n",
      "episode: 1525/5000, score: -132.03909983691366\n",
      "episode: 1526/5000, score: -71.87840901830872\n",
      "episode: 1527/5000, score: -124.38217927026368\n",
      "episode: 1528/5000, score: -181.6549136895922\n",
      "episode: 1529/5000, score: -170.93878689654065\n",
      "episode: 1530/5000, score: -140.0210893390413\n",
      "episode: 1531/5000, score: -211.3263343754825\n",
      "episode: 1532/5000, score: -140.74593446663923\n",
      "episode: 1533/5000, score: -112.80069924742\n",
      "episode: 1534/5000, score: -147.0238285773747\n",
      "episode: 1535/5000, score: -145.53015441097992\n",
      "episode: 1536/5000, score: -97.09528971219075\n",
      "episode: 1537/5000, score: -151.64457806159677\n",
      "episode: 1538/5000, score: -174.64876014069162\n",
      "episode: 1539/5000, score: -163.22887876793908\n",
      "episode: 1540/5000, score: -146.5116712539874\n",
      "episode: 1541/5000, score: -111.95877459286665\n",
      "episode: 1542/5000, score: -129.8831623995959\n",
      "episode: 1543/5000, score: -83.7756203875098\n",
      "episode: 1544/5000, score: -173.38522716208666\n",
      "episode: 1545/5000, score: -109.02995304563474\n",
      "episode: 1546/5000, score: -117.44167576213366\n",
      "episode: 1547/5000, score: -176.57477378209288\n",
      "episode: 1548/5000, score: -101.50725677239913\n",
      "episode: 1549/5000, score: -133.92828832218981\n",
      "episode: 1550/5000, score: -159.30266741482413\n",
      "episode: 1551/5000, score: -187.8541648402295\n",
      "episode: 1552/5000, score: -182.57325591942123\n",
      "episode: 1553/5000, score: -189.10454898742208\n",
      "episode: 1554/5000, score: -37.35639237129146\n",
      "episode: 1555/5000, score: -167.00463493296334\n",
      "episode: 1556/5000, score: -185.6535473971311\n",
      "episode: 1557/5000, score: -192.7541188589089\n",
      "episode: 1558/5000, score: -79.64549023697144\n",
      "episode: 1559/5000, score: -189.16997203730742\n",
      "episode: 1560/5000, score: -154.79187870455496\n",
      "episode: 1561/5000, score: -169.52820707929786\n",
      "episode: 1562/5000, score: -192.82978128123167\n",
      "episode: 1563/5000, score: -160.7186776937453\n",
      "episode: 1564/5000, score: -135.68785669907356\n",
      "episode: 1565/5000, score: -162.9525505247368\n",
      "episode: 1566/5000, score: -70.73554548240834\n",
      "episode: 1567/5000, score: -168.5048969435227\n",
      "episode: 1568/5000, score: -142.7293186825995\n",
      "episode: 1569/5000, score: -172.1192051687853\n",
      "episode: 1570/5000, score: -167.2764423156342\n",
      "episode: 1571/5000, score: -153.27927979562904\n",
      "episode: 1572/5000, score: -166.8780063586291\n",
      "episode: 1573/5000, score: -102.52853048837453\n",
      "episode: 1574/5000, score: -97.97485808313012\n",
      "episode: 1575/5000, score: -124.56034292589702\n",
      "episode: 1576/5000, score: -189.14126144201606\n",
      "episode: 1577/5000, score: -137.95495399469715\n",
      "episode: 1578/5000, score: -215.0878519297684\n",
      "episode: 1579/5000, score: -176.03005876158517\n",
      "episode: 1580/5000, score: -148.84693760977194\n",
      "episode: 1581/5000, score: -246.49379792593405\n",
      "episode: 1582/5000, score: -174.47549160674177\n",
      "episode: 1583/5000, score: -187.19124130322118\n",
      "episode: 1584/5000, score: -165.84371422122865\n",
      "episode: 1585/5000, score: -106.95054970235526\n",
      "episode: 1586/5000, score: -211.18779083522156\n",
      "episode: 1587/5000, score: -220.3788025062915\n",
      "episode: 1588/5000, score: -155.77213771711143\n",
      "episode: 1589/5000, score: -98.99936292464041\n",
      "episode: 1590/5000, score: -134.9752294900335\n",
      "episode: 1591/5000, score: -123.78105587780581\n",
      "episode: 1592/5000, score: -189.10230304672663\n",
      "episode: 1593/5000, score: -99.7555541064624\n",
      "episode: 1594/5000, score: -85.56892282165008\n",
      "episode: 1595/5000, score: -179.9245037851805\n",
      "episode: 1596/5000, score: -186.51840748025583\n",
      "episode: 1597/5000, score: -149.32301052194185\n",
      "episode: 1598/5000, score: -188.18440280619149\n",
      "episode: 1599/5000, score: -82.95832632440532\n",
      "episode: 1600/5000, score: -99.9260983830126\n",
      "episode: 1601/5000, score: -99.13363603786149\n",
      "episode: 1602/5000, score: -157.31139393993303\n",
      "episode: 1603/5000, score: -195.29093589740398\n",
      "episode: 1604/5000, score: -186.15824891915628\n",
      "episode: 1605/5000, score: -150.69786312815185\n",
      "episode: 1606/5000, score: -152.23612553006598\n",
      "episode: 1607/5000, score: -161.22620112435052\n",
      "episode: 1608/5000, score: -132.29791258450896\n",
      "episode: 1609/5000, score: -167.90680306475764\n",
      "episode: 1610/5000, score: -147.68259587415724\n",
      "episode: 1611/5000, score: -129.26338097471086\n",
      "episode: 1612/5000, score: -143.01922755473507\n",
      "episode: 1613/5000, score: -140.39513810937154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1614/5000, score: -172.48351784236954\n",
      "episode: 1615/5000, score: -163.94880963400576\n",
      "episode: 1616/5000, score: -147.47010470008547\n",
      "episode: 1617/5000, score: -109.01189924963296\n",
      "episode: 1618/5000, score: -163.64759315511503\n",
      "episode: 1619/5000, score: -142.92804399248544\n",
      "episode: 1620/5000, score: -181.0654874411369\n",
      "episode: 1621/5000, score: -148.52093733298605\n",
      "episode: 1622/5000, score: -164.36487524162115\n",
      "episode: 1623/5000, score: -127.94188613944989\n",
      "episode: 1624/5000, score: -172.42616785874677\n",
      "episode: 1625/5000, score: -134.02685522338948\n",
      "episode: 1626/5000, score: -136.20084600818393\n",
      "episode: 1627/5000, score: -155.93928729595535\n",
      "episode: 1628/5000, score: -103.17359263963033\n",
      "episode: 1629/5000, score: -137.60913297245077\n",
      "episode: 1630/5000, score: -153.55591036387142\n",
      "episode: 1631/5000, score: -156.30152962201146\n",
      "episode: 1632/5000, score: -143.54224207607203\n",
      "episode: 1633/5000, score: -126.12156311288985\n",
      "episode: 1634/5000, score: -132.38501203139074\n",
      "episode: 1635/5000, score: -119.12225646554097\n",
      "episode: 1636/5000, score: -188.3934357663316\n",
      "episode: 1637/5000, score: -181.36635696129926\n",
      "episode: 1638/5000, score: -175.77719935009273\n",
      "episode: 1639/5000, score: -118.20742376966199\n",
      "episode: 1640/5000, score: -88.79354851574311\n",
      "episode: 1641/5000, score: -119.66921888809439\n",
      "episode: 1642/5000, score: -134.03249841579733\n",
      "episode: 1643/5000, score: -174.21643075295123\n",
      "episode: 1644/5000, score: -141.21513705429058\n",
      "episode: 1645/5000, score: -129.5777058851874\n",
      "episode: 1646/5000, score: -165.42065892857912\n",
      "episode: 1647/5000, score: -177.55646536689179\n",
      "episode: 1648/5000, score: -164.18209041733738\n",
      "episode: 1649/5000, score: -124.90157588881081\n",
      "episode: 1650/5000, score: -114.81280957206296\n",
      "episode: 1651/5000, score: -191.29761531127204\n",
      "episode: 1652/5000, score: -177.14382577204256\n",
      "episode: 1653/5000, score: -80.75646976990268\n",
      "episode: 1654/5000, score: -157.49144033800795\n",
      "episode: 1655/5000, score: -190.82829497196022\n",
      "episode: 1656/5000, score: -145.1484790934072\n",
      "episode: 1657/5000, score: -126.33908565516924\n",
      "episode: 1658/5000, score: -62.20632839624672\n",
      "episode: 1659/5000, score: -143.90791038547877\n",
      "episode: 1660/5000, score: -137.04601752879267\n",
      "episode: 1661/5000, score: -180.67354114618203\n",
      "episode: 1662/5000, score: -129.73884398077098\n",
      "episode: 1663/5000, score: -94.0604833315478\n",
      "episode: 1664/5000, score: -115.03590918194361\n",
      "episode: 1665/5000, score: -69.37743891036058\n",
      "episode: 1666/5000, score: -128.5488023381967\n",
      "episode: 1667/5000, score: -184.82357706199232\n",
      "episode: 1668/5000, score: -173.24725706199172\n",
      "episode: 1669/5000, score: -205.72934771637227\n",
      "episode: 1670/5000, score: -198.7346337727123\n",
      "episode: 1671/5000, score: -152.98937851490615\n",
      "episode: 1672/5000, score: -106.34478404342273\n",
      "episode: 1673/5000, score: -172.2636809685746\n",
      "episode: 1674/5000, score: -105.56197610841969\n",
      "episode: 1675/5000, score: -174.35892530719198\n",
      "episode: 1676/5000, score: -209.82193829835808\n",
      "episode: 1677/5000, score: -152.32347304417127\n",
      "episode: 1678/5000, score: -173.39713236459312\n",
      "episode: 1679/5000, score: -125.27969766407985\n",
      "episode: 1680/5000, score: -184.26818856960193\n",
      "episode: 1681/5000, score: -181.94865756453822\n",
      "episode: 1682/5000, score: -80.20260044483688\n",
      "episode: 1683/5000, score: -137.31168810873515\n",
      "episode: 1684/5000, score: -164.85552570166482\n",
      "episode: 1685/5000, score: -146.26760672660467\n",
      "episode: 1686/5000, score: -140.87930197029888\n",
      "episode: 1687/5000, score: -178.3342552373754\n",
      "episode: 1688/5000, score: -174.49531849085244\n",
      "episode: 1689/5000, score: -198.43474853262066\n",
      "episode: 1690/5000, score: -185.6892015344463\n",
      "episode: 1691/5000, score: -183.7362056210989\n",
      "episode: 1692/5000, score: -169.71799358160024\n",
      "episode: 1693/5000, score: -186.73863579724173\n",
      "episode: 1694/5000, score: -209.1143470131331\n",
      "episode: 1695/5000, score: -186.4691766758644\n",
      "episode: 1696/5000, score: -145.60733457452633\n",
      "episode: 1697/5000, score: -174.45714631956164\n",
      "episode: 1698/5000, score: -176.15044963575176\n",
      "episode: 1699/5000, score: -152.34279713994076\n",
      "episode: 1700/5000, score: -193.463778962481\n",
      "episode: 1701/5000, score: -199.9616691610051\n",
      "episode: 1702/5000, score: -136.60428919129103\n",
      "episode: 1703/5000, score: -184.95560314302554\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-be94269e4d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# train the agent with the experience of the episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# print the score and break out of the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f5a21bbcac08>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, total_step, batch_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize gym environment and the agent\n",
    "env = gym.make('LunarLander-v2')\n",
    "observation_space = env.observation_space.shape[0]\n",
    "print('Observation space: ', observation_space)\n",
    "action_space = env.action_space.n\n",
    "print('Action space: ', action_space)\n",
    "agent = DDQNAgent(observation_space, action_space)\n",
    "episodes = 5000\n",
    "total_step = 0\n",
    "# Iterate the game\n",
    "for e in range(episodes):\n",
    "    # reset state in the beginning of each game\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, observation_space])\n",
    "    # time_t represents each frame of the game\n",
    "    # Our goal is to keep the pole upright as long as possible until score of 500\n",
    "    # the more time_t the more score\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        total_step += 1\n",
    "        # turn this on if you want to render\n",
    "        env.render()\n",
    "        # Decide action\n",
    "        action = agent.act(state)\n",
    "        # Advance the game to the next frame based on the action.\n",
    "        # Reward is 1 for every frame the pole survived\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        next_state = np.reshape(next_state, [1, observation_space])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        # train the agent with the experience of the episode\n",
    "        agent.replay(total_step, 32)\n",
    "        if done:\n",
    "            # print the score and break out of the loop\n",
    "            print(\"episode: {}/{}, score: {}\"\n",
    "                  .format(e, episodes, score))\n",
    "            break\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=1000000)\n",
    "        self.gamma = 0.99    # discount rate\n",
    "        self.rho = 0.95\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        frame_input = Input(self.state_size)\n",
    "        action_input = Input((self.action_size,))\n",
    "        normalized = Lambda(lambda x: x / 255.0)(frame_input)\n",
    "        conv_1 = Convolution2D(16, (8, 8), strides=(4, 4), activation='relu')(normalized)\n",
    "        conv_2 = Convolution2D(32, (4, 4), strides=(2, 2), activation='relu')(conv_1)\n",
    "        conv_flattened = Flatten()(conv_2)\n",
    "        hidden = Dense(256, activation='relu')(conv_flattened)\n",
    "        output = Dense(self.action_size)(hidden)\n",
    "        filtered_output = Multiply()([output, action_input])\n",
    "        model = Model(inputs=[frame_input, action_input], outputs=filtered_output)\n",
    "        optimizer = RMSprop(lr=self.learning_rate, rho=self.rho, epsilon=self.epsilon)\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "class DDQNAgent:\n",
    "    def __init__(self, game, state_size, action_size):\n",
    "        self.game = game\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=1000000)\n",
    "        self.gamma = 0.99    # discount rate\n",
    "        self.rho = 0.95\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.000001059\n",
    "        self.learning_rate = 0.00025\n",
    "        self.training_step = 4\n",
    "        self.target_update_frequency = 40000\n",
    "        self.save_frequency = 30000\n",
    "        self.replay_start_size = 50000\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.reset_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(32, 8, input_shape=self.state_size, strides=(4, 4), activation='relu'))\n",
    "        model.add(Convolution2D(64, 4, input_shape=self.state_size, strides=(2, 2), activation='relu'))\n",
    "        model.add(Convolution2D(64, 3, input_shape=self.state_size, strides=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        optimizer = RMSprop(lr=self.learning_rate, rho=self.rho, epsilon=0.01)\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    def save_model(self, step):\n",
    "        print('Saving model: atari' + str(self.game) + str(step) + '.h5')\n",
    "        self.model.save('atari' + str(self.game) + str(step) + '.h5')\n",
    "        \n",
    "    def reset_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon or len(self.memory) < self.replay_start_size:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(np.expand_dims(np.asarray(state), axis=0))\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, step, batch_size):\n",
    "        if len(self.memory) < self.replay_start_size:\n",
    "            return\n",
    "        if step % self.training_step == 0:\n",
    "            minibatch = random.sample(self.memory, batch_size)\n",
    "            for state, action, reward, next_state, done in minibatch:\n",
    "                state = np.expand_dims(np.asarray(state), axis=0)\n",
    "                next_state = np.expand_dims(np.asarray(next_state), axis=0)\n",
    "                target = reward\n",
    "                if not done:\n",
    "                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\n",
    "                target_f = self.model.predict(state)\n",
    "                target_f[0][action] = target\n",
    "                self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if step % self.target_update_frequency == 0:\n",
    "            self.reset_target_model()\n",
    "        if step % self.save_frequency == 0:\n",
    "            self.save_model(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    return np.expand_dims(to_grayscale(downsample(img)), axis=2)\n",
    "\n",
    "def transform_reward(reward):\n",
    "    return np.sign(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  (84, 84, 4)\n",
      "Action space:  6\n",
      "episode: 0/100000, steps: 850, total steps: 850, score: -21.0\n",
      "episode: 1/100000, steps: 762, total steps: 1612, score: -21.0\n",
      "episode: 2/100000, steps: 1012, total steps: 2624, score: -19.0\n",
      "episode: 3/100000, steps: 942, total steps: 3566, score: -21.0\n",
      "episode: 4/100000, steps: 878, total steps: 4444, score: -20.0\n",
      "episode: 5/100000, steps: 1002, total steps: 5446, score: -21.0\n",
      "episode: 6/100000, steps: 1146, total steps: 6592, score: -20.0\n",
      "episode: 7/100000, steps: 896, total steps: 7488, score: -20.0\n",
      "episode: 8/100000, steps: 997, total steps: 8485, score: -18.0\n",
      "episode: 9/100000, steps: 976, total steps: 9461, score: -20.0\n",
      "episode: 10/100000, steps: 850, total steps: 10311, score: -21.0\n",
      "episode: 11/100000, steps: 822, total steps: 11133, score: -21.0\n",
      "episode: 12/100000, steps: 836, total steps: 11969, score: -20.0\n",
      "episode: 13/100000, steps: 790, total steps: 12759, score: -21.0\n",
      "episode: 14/100000, steps: 991, total steps: 13750, score: -19.0\n",
      "episode: 15/100000, steps: 1019, total steps: 14769, score: -20.0\n",
      "episode: 16/100000, steps: 822, total steps: 15591, score: -21.0\n",
      "episode: 17/100000, steps: 822, total steps: 16413, score: -21.0\n",
      "episode: 18/100000, steps: 824, total steps: 17237, score: -21.0\n",
      "episode: 19/100000, steps: 964, total steps: 18201, score: -20.0\n",
      "episode: 20/100000, steps: 762, total steps: 18963, score: -21.0\n",
      "episode: 21/100000, steps: 917, total steps: 19880, score: -20.0\n",
      "episode: 22/100000, steps: 824, total steps: 20704, score: -21.0\n",
      "episode: 23/100000, steps: 790, total steps: 21494, score: -21.0\n",
      "episode: 24/100000, steps: 1004, total steps: 22498, score: -21.0\n",
      "episode: 25/100000, steps: 1030, total steps: 23528, score: -18.0\n",
      "episode: 26/100000, steps: 1285, total steps: 24813, score: -20.0\n",
      "episode: 27/100000, steps: 850, total steps: 25663, score: -21.0\n",
      "episode: 28/100000, steps: 930, total steps: 26593, score: -21.0\n",
      "episode: 29/100000, steps: 762, total steps: 27355, score: -21.0\n",
      "episode: 30/100000, steps: 978, total steps: 28333, score: -20.0\n",
      "episode: 31/100000, steps: 1009, total steps: 29342, score: -19.0\n",
      "episode: 32/100000, steps: 996, total steps: 30338, score: -18.0\n",
      "episode: 33/100000, steps: 762, total steps: 31100, score: -21.0\n",
      "episode: 34/100000, steps: 947, total steps: 32047, score: -20.0\n",
      "episode: 35/100000, steps: 1021, total steps: 33068, score: -19.0\n",
      "episode: 36/100000, steps: 864, total steps: 33932, score: -20.0\n",
      "episode: 37/100000, steps: 880, total steps: 34812, score: -21.0\n",
      "episode: 38/100000, steps: 912, total steps: 35724, score: -21.0\n",
      "episode: 39/100000, steps: 762, total steps: 36486, score: -21.0\n",
      "episode: 40/100000, steps: 854, total steps: 37340, score: -21.0\n",
      "episode: 41/100000, steps: 968, total steps: 38308, score: -21.0\n",
      "episode: 42/100000, steps: 882, total steps: 39190, score: -21.0\n",
      "episode: 43/100000, steps: 822, total steps: 40012, score: -21.0\n",
      "episode: 44/100000, steps: 943, total steps: 40955, score: -21.0\n",
      "episode: 45/100000, steps: 918, total steps: 41873, score: -19.0\n",
      "episode: 46/100000, steps: 979, total steps: 42852, score: -20.0\n",
      "episode: 47/100000, steps: 929, total steps: 43781, score: -20.0\n",
      "episode: 48/100000, steps: 1023, total steps: 44804, score: -19.0\n",
      "episode: 49/100000, steps: 762, total steps: 45566, score: -21.0\n",
      "episode: 50/100000, steps: 1169, total steps: 46735, score: -20.0\n",
      "episode: 51/100000, steps: 882, total steps: 47617, score: -21.0\n",
      "episode: 52/100000, steps: 780, total steps: 48397, score: -21.0\n",
      "episode: 53/100000, steps: 781, total steps: 49178, score: -21.0\n",
      "episode: 54/100000, steps: 952, total steps: 50130, score: -20.0\n",
      "episode: 55/100000, steps: 882, total steps: 51012, score: -21.0\n",
      "episode: 56/100000, steps: 1085, total steps: 52097, score: -20.0\n",
      "episode: 57/100000, steps: 762, total steps: 52859, score: -21.0\n",
      "episode: 58/100000, steps: 930, total steps: 53789, score: -21.0\n",
      "episode: 59/100000, steps: 1084, total steps: 54873, score: -19.0\n",
      "episode: 60/100000, steps: 790, total steps: 55663, score: -21.0\n",
      "episode: 61/100000, steps: 1027, total steps: 56690, score: -20.0\n",
      "episode: 62/100000, steps: 870, total steps: 57560, score: -21.0\n",
      "episode: 63/100000, steps: 976, total steps: 58536, score: -20.0\n",
      "episode: 64/100000, steps: 852, total steps: 59388, score: -21.0\n",
      "Saving model: atariPong60000.h5\n",
      "episode: 65/100000, steps: 1347, total steps: 60735, score: -16.0\n",
      "episode: 66/100000, steps: 1006, total steps: 61741, score: -20.0\n",
      "episode: 67/100000, steps: 762, total steps: 62503, score: -21.0\n",
      "episode: 68/100000, steps: 852, total steps: 63355, score: -21.0\n",
      "episode: 69/100000, steps: 1173, total steps: 64528, score: -18.0\n",
      "episode: 70/100000, steps: 928, total steps: 65456, score: -20.0\n",
      "episode: 71/100000, steps: 930, total steps: 66386, score: -20.0\n",
      "episode: 72/100000, steps: 781, total steps: 67167, score: -21.0\n",
      "episode: 73/100000, steps: 1113, total steps: 68280, score: -19.0\n",
      "episode: 74/100000, steps: 1228, total steps: 69508, score: -18.0\n",
      "episode: 75/100000, steps: 790, total steps: 70298, score: -21.0\n",
      "episode: 76/100000, steps: 854, total steps: 71152, score: -20.0\n",
      "episode: 77/100000, steps: 984, total steps: 72136, score: -20.0\n",
      "episode: 78/100000, steps: 946, total steps: 73082, score: -19.0\n",
      "episode: 79/100000, steps: 791, total steps: 73873, score: -21.0\n",
      "episode: 80/100000, steps: 928, total steps: 74801, score: -20.0\n",
      "episode: 81/100000, steps: 973, total steps: 75774, score: -21.0\n",
      "episode: 82/100000, steps: 1005, total steps: 76779, score: -20.0\n",
      "episode: 83/100000, steps: 915, total steps: 77694, score: -20.0\n",
      "episode: 84/100000, steps: 822, total steps: 78516, score: -21.0\n",
      "episode: 85/100000, steps: 781, total steps: 79297, score: -21.0\n",
      "episode: 86/100000, steps: 809, total steps: 80106, score: -21.0\n",
      "episode: 87/100000, steps: 998, total steps: 81104, score: -21.0\n",
      "episode: 88/100000, steps: 1034, total steps: 82138, score: -19.0\n",
      "episode: 89/100000, steps: 931, total steps: 83069, score: -21.0\n",
      "episode: 90/100000, steps: 982, total steps: 84051, score: -20.0\n",
      "episode: 91/100000, steps: 919, total steps: 84970, score: -20.0\n",
      "episode: 92/100000, steps: 1038, total steps: 86008, score: -20.0\n",
      "episode: 93/100000, steps: 881, total steps: 86889, score: -21.0\n",
      "episode: 94/100000, steps: 852, total steps: 87741, score: -21.0\n",
      "episode: 95/100000, steps: 930, total steps: 88671, score: -20.0\n",
      "episode: 96/100000, steps: 822, total steps: 89493, score: -21.0\n",
      "Saving model: atariPong90000.h5\n",
      "episode: 97/100000, steps: 1041, total steps: 90534, score: -21.0\n",
      "episode: 98/100000, steps: 781, total steps: 91315, score: -21.0\n",
      "episode: 99/100000, steps: 1020, total steps: 92335, score: -20.0\n",
      "episode: 100/100000, steps: 852, total steps: 93187, score: -21.0\n",
      "episode: 101/100000, steps: 1124, total steps: 94311, score: -20.0\n",
      "episode: 102/100000, steps: 936, total steps: 95247, score: -19.0\n",
      "episode: 103/100000, steps: 762, total steps: 96009, score: -21.0\n",
      "episode: 104/100000, steps: 878, total steps: 96887, score: -21.0\n",
      "episode: 105/100000, steps: 841, total steps: 97728, score: -21.0\n",
      "episode: 106/100000, steps: 914, total steps: 98642, score: -20.0\n",
      "episode: 107/100000, steps: 1185, total steps: 99827, score: -18.0\n",
      "episode: 108/100000, steps: 822, total steps: 100649, score: -21.0\n",
      "episode: 109/100000, steps: 822, total steps: 101471, score: -21.0\n",
      "episode: 110/100000, steps: 988, total steps: 102459, score: -20.0\n",
      "episode: 111/100000, steps: 980, total steps: 103439, score: -19.0\n",
      "episode: 112/100000, steps: 868, total steps: 104307, score: -20.0\n",
      "episode: 113/100000, steps: 762, total steps: 105069, score: -21.0\n",
      "episode: 114/100000, steps: 869, total steps: 105938, score: -20.0\n",
      "episode: 115/100000, steps: 762, total steps: 106700, score: -21.0\n",
      "episode: 116/100000, steps: 820, total steps: 107520, score: -21.0\n",
      "episode: 117/100000, steps: 882, total steps: 108402, score: -21.0\n",
      "episode: 118/100000, steps: 898, total steps: 109300, score: -20.0\n",
      "episode: 119/100000, steps: 762, total steps: 110062, score: -21.0\n",
      "episode: 120/100000, steps: 883, total steps: 110945, score: -21.0\n",
      "episode: 121/100000, steps: 921, total steps: 111866, score: -20.0\n",
      "episode: 122/100000, steps: 762, total steps: 112628, score: -21.0\n",
      "episode: 123/100000, steps: 886, total steps: 113514, score: -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 124/100000, steps: 1425, total steps: 114939, score: -15.0\n",
      "episode: 125/100000, steps: 809, total steps: 115748, score: -21.0\n",
      "episode: 126/100000, steps: 896, total steps: 116644, score: -20.0\n",
      "episode: 127/100000, steps: 989, total steps: 117633, score: -20.0\n",
      "episode: 128/100000, steps: 961, total steps: 118594, score: -20.0\n",
      "episode: 129/100000, steps: 822, total steps: 119416, score: -21.0\n",
      "Saving model: atariPong120000.h5\n",
      "episode: 130/100000, steps: 880, total steps: 120296, score: -21.0\n",
      "episode: 131/100000, steps: 1006, total steps: 121302, score: -20.0\n",
      "episode: 132/100000, steps: 822, total steps: 122124, score: -21.0\n",
      "episode: 133/100000, steps: 979, total steps: 123103, score: -20.0\n",
      "episode: 134/100000, steps: 884, total steps: 123987, score: -21.0\n",
      "episode: 135/100000, steps: 868, total steps: 124855, score: -20.0\n",
      "episode: 136/100000, steps: 868, total steps: 125723, score: -20.0\n",
      "episode: 137/100000, steps: 970, total steps: 126693, score: -21.0\n",
      "episode: 138/100000, steps: 1038, total steps: 127731, score: -20.0\n",
      "episode: 139/100000, steps: 840, total steps: 128571, score: -21.0\n",
      "episode: 140/100000, steps: 841, total steps: 129412, score: -21.0\n",
      "episode: 141/100000, steps: 822, total steps: 130234, score: -21.0\n",
      "episode: 142/100000, steps: 878, total steps: 131112, score: -21.0\n",
      "episode: 143/100000, steps: 951, total steps: 132063, score: -21.0\n",
      "episode: 144/100000, steps: 836, total steps: 132899, score: -20.0\n",
      "episode: 145/100000, steps: 958, total steps: 133857, score: -21.0\n",
      "episode: 146/100000, steps: 1059, total steps: 134916, score: -18.0\n",
      "episode: 147/100000, steps: 790, total steps: 135706, score: -21.0\n",
      "episode: 148/100000, steps: 818, total steps: 136524, score: -21.0\n",
      "episode: 149/100000, steps: 1021, total steps: 137545, score: -18.0\n",
      "episode: 150/100000, steps: 762, total steps: 138307, score: -21.0\n",
      "episode: 151/100000, steps: 874, total steps: 139181, score: -21.0\n",
      "episode: 152/100000, steps: 910, total steps: 140091, score: -21.0\n",
      "episode: 153/100000, steps: 762, total steps: 140853, score: -21.0\n",
      "episode: 154/100000, steps: 938, total steps: 141791, score: -21.0\n",
      "episode: 155/100000, steps: 1007, total steps: 142798, score: -20.0\n",
      "episode: 156/100000, steps: 1194, total steps: 143992, score: -17.0\n",
      "episode: 157/100000, steps: 1035, total steps: 145027, score: -20.0\n",
      "episode: 158/100000, steps: 1022, total steps: 146049, score: -20.0\n",
      "episode: 159/100000, steps: 762, total steps: 146811, score: -21.0\n",
      "episode: 160/100000, steps: 880, total steps: 147691, score: -21.0\n",
      "episode: 161/100000, steps: 900, total steps: 148591, score: -21.0\n",
      "episode: 162/100000, steps: 1065, total steps: 149656, score: -20.0\n",
      "Saving model: atariPong150000.h5\n",
      "episode: 163/100000, steps: 1021, total steps: 150677, score: -21.0\n",
      "episode: 164/100000, steps: 900, total steps: 151577, score: -20.0\n",
      "episode: 165/100000, steps: 880, total steps: 152457, score: -21.0\n",
      "episode: 166/100000, steps: 762, total steps: 153219, score: -21.0\n",
      "episode: 167/100000, steps: 841, total steps: 154060, score: -21.0\n",
      "episode: 168/100000, steps: 1044, total steps: 155104, score: -20.0\n",
      "episode: 169/100000, steps: 781, total steps: 155885, score: -21.0\n",
      "episode: 170/100000, steps: 982, total steps: 156867, score: -19.0\n",
      "episode: 171/100000, steps: 901, total steps: 157768, score: -20.0\n",
      "episode: 172/100000, steps: 901, total steps: 158669, score: -20.0\n",
      "episode: 173/100000, steps: 1127, total steps: 159796, score: -20.0\n",
      "episode: 174/100000, steps: 1086, total steps: 160882, score: -18.0\n",
      "episode: 175/100000, steps: 810, total steps: 161692, score: -21.0\n",
      "episode: 176/100000, steps: 902, total steps: 162594, score: -21.0\n",
      "episode: 177/100000, steps: 822, total steps: 163416, score: -21.0\n",
      "episode: 178/100000, steps: 912, total steps: 164328, score: -21.0\n",
      "episode: 179/100000, steps: 888, total steps: 165216, score: -20.0\n",
      "episode: 180/100000, steps: 924, total steps: 166140, score: -20.0\n",
      "episode: 181/100000, steps: 985, total steps: 167125, score: -20.0\n",
      "episode: 182/100000, steps: 870, total steps: 167995, score: -20.0\n",
      "episode: 183/100000, steps: 878, total steps: 168873, score: -21.0\n",
      "episode: 184/100000, steps: 1006, total steps: 169879, score: -20.0\n",
      "episode: 185/100000, steps: 1040, total steps: 170919, score: -19.0\n",
      "episode: 186/100000, steps: 1023, total steps: 171942, score: -20.0\n",
      "episode: 187/100000, steps: 1078, total steps: 173020, score: -19.0\n",
      "episode: 188/100000, steps: 900, total steps: 173920, score: -20.0\n",
      "episode: 189/100000, steps: 852, total steps: 174772, score: -21.0\n",
      "episode: 190/100000, steps: 857, total steps: 175629, score: -20.0\n",
      "episode: 191/100000, steps: 994, total steps: 176623, score: -20.0\n",
      "episode: 192/100000, steps: 1007, total steps: 177630, score: -20.0\n",
      "episode: 193/100000, steps: 856, total steps: 178486, score: -21.0\n",
      "episode: 194/100000, steps: 912, total steps: 179398, score: -21.0\n",
      "Saving model: atariPong180000.h5\n",
      "episode: 195/100000, steps: 973, total steps: 180371, score: -21.0\n",
      "episode: 196/100000, steps: 841, total steps: 181212, score: -21.0\n",
      "episode: 197/100000, steps: 944, total steps: 182156, score: -21.0\n",
      "episode: 198/100000, steps: 958, total steps: 183114, score: -20.0\n",
      "episode: 199/100000, steps: 1007, total steps: 184121, score: -20.0\n",
      "episode: 200/100000, steps: 823, total steps: 184944, score: -21.0\n",
      "episode: 201/100000, steps: 1034, total steps: 185978, score: -20.0\n",
      "episode: 202/100000, steps: 882, total steps: 186860, score: -21.0\n",
      "episode: 203/100000, steps: 762, total steps: 187622, score: -21.0\n",
      "episode: 204/100000, steps: 929, total steps: 188551, score: -20.0\n",
      "episode: 205/100000, steps: 822, total steps: 189373, score: -21.0\n",
      "episode: 206/100000, steps: 948, total steps: 190321, score: -20.0\n",
      "episode: 207/100000, steps: 762, total steps: 191083, score: -21.0\n",
      "episode: 208/100000, steps: 929, total steps: 192012, score: -20.0\n",
      "episode: 209/100000, steps: 952, total steps: 192964, score: -20.0\n",
      "episode: 210/100000, steps: 1112, total steps: 194076, score: -20.0\n",
      "episode: 211/100000, steps: 843, total steps: 194919, score: -21.0\n",
      "episode: 212/100000, steps: 977, total steps: 195896, score: -20.0\n",
      "episode: 213/100000, steps: 840, total steps: 196736, score: -20.0\n",
      "episode: 214/100000, steps: 913, total steps: 197649, score: -21.0\n",
      "episode: 215/100000, steps: 997, total steps: 198646, score: -19.0\n",
      "episode: 216/100000, steps: 997, total steps: 199643, score: -19.0\n",
      "episode: 217/100000, steps: 762, total steps: 200405, score: -21.0\n",
      "episode: 218/100000, steps: 977, total steps: 201382, score: -20.0\n",
      "episode: 219/100000, steps: 938, total steps: 202320, score: -19.0\n",
      "episode: 220/100000, steps: 907, total steps: 203227, score: -21.0\n",
      "episode: 221/100000, steps: 864, total steps: 204091, score: -20.0\n",
      "episode: 222/100000, steps: 882, total steps: 204973, score: -21.0\n",
      "episode: 223/100000, steps: 841, total steps: 205814, score: -20.0\n",
      "episode: 224/100000, steps: 912, total steps: 206726, score: -21.0\n",
      "episode: 225/100000, steps: 762, total steps: 207488, score: -21.0\n",
      "episode: 226/100000, steps: 1053, total steps: 208541, score: -19.0\n",
      "episode: 227/100000, steps: 790, total steps: 209331, score: -21.0\n",
      "Saving model: atariPong210000.h5\n",
      "episode: 228/100000, steps: 972, total steps: 210303, score: -19.0\n",
      "episode: 229/100000, steps: 888, total steps: 211191, score: -20.0\n",
      "episode: 230/100000, steps: 1060, total steps: 212251, score: -21.0\n",
      "episode: 231/100000, steps: 850, total steps: 213101, score: -21.0\n",
      "episode: 232/100000, steps: 1017, total steps: 214118, score: -19.0\n",
      "episode: 233/100000, steps: 994, total steps: 215112, score: -20.0\n",
      "episode: 234/100000, steps: 996, total steps: 216108, score: -18.0\n",
      "episode: 235/100000, steps: 762, total steps: 216870, score: -21.0\n",
      "episode: 236/100000, steps: 911, total steps: 217781, score: -21.0\n",
      "episode: 237/100000, steps: 991, total steps: 218772, score: -20.0\n",
      "episode: 238/100000, steps: 854, total steps: 219626, score: -20.0\n",
      "episode: 239/100000, steps: 822, total steps: 220448, score: -21.0\n",
      "episode: 240/100000, steps: 871, total steps: 221319, score: -21.0\n",
      "episode: 241/100000, steps: 852, total steps: 222171, score: -21.0\n",
      "episode: 242/100000, steps: 911, total steps: 223082, score: -21.0\n",
      "episode: 243/100000, steps: 982, total steps: 224064, score: -19.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 244/100000, steps: 866, total steps: 224930, score: -21.0\n",
      "episode: 245/100000, steps: 781, total steps: 225711, score: -21.0\n",
      "episode: 246/100000, steps: 912, total steps: 226623, score: -21.0\n",
      "episode: 247/100000, steps: 762, total steps: 227385, score: -21.0\n",
      "episode: 248/100000, steps: 790, total steps: 228175, score: -21.0\n",
      "episode: 249/100000, steps: 973, total steps: 229148, score: -20.0\n",
      "episode: 250/100000, steps: 824, total steps: 229972, score: -21.0\n",
      "episode: 251/100000, steps: 762, total steps: 230734, score: -21.0\n",
      "episode: 252/100000, steps: 824, total steps: 231558, score: -21.0\n",
      "episode: 253/100000, steps: 877, total steps: 232435, score: -20.0\n",
      "episode: 254/100000, steps: 809, total steps: 233244, score: -21.0\n",
      "episode: 255/100000, steps: 1034, total steps: 234278, score: -20.0\n",
      "episode: 256/100000, steps: 988, total steps: 235266, score: -20.0\n",
      "episode: 257/100000, steps: 822, total steps: 236088, score: -21.0\n",
      "episode: 258/100000, steps: 851, total steps: 236939, score: -21.0\n",
      "episode: 259/100000, steps: 822, total steps: 237761, score: -21.0\n",
      "episode: 260/100000, steps: 790, total steps: 238551, score: -21.0\n",
      "episode: 261/100000, steps: 1054, total steps: 239605, score: -19.0\n",
      "Saving model: atariPong240000.h5\n",
      "episode: 262/100000, steps: 1083, total steps: 240688, score: -19.0\n",
      "episode: 263/100000, steps: 1038, total steps: 241726, score: -19.0\n",
      "episode: 264/100000, steps: 841, total steps: 242567, score: -21.0\n",
      "episode: 265/100000, steps: 824, total steps: 243391, score: -21.0\n",
      "episode: 266/100000, steps: 1081, total steps: 244472, score: -20.0\n",
      "episode: 267/100000, steps: 877, total steps: 245349, score: -20.0\n",
      "episode: 268/100000, steps: 809, total steps: 246158, score: -21.0\n",
      "episode: 269/100000, steps: 1133, total steps: 247291, score: -18.0\n",
      "episode: 270/100000, steps: 886, total steps: 248177, score: -21.0\n",
      "episode: 271/100000, steps: 822, total steps: 248999, score: -21.0\n",
      "episode: 272/100000, steps: 960, total steps: 249959, score: -20.0\n",
      "episode: 273/100000, steps: 884, total steps: 250843, score: -21.0\n",
      "episode: 274/100000, steps: 781, total steps: 251624, score: -21.0\n",
      "episode: 275/100000, steps: 972, total steps: 252596, score: -21.0\n",
      "episode: 276/100000, steps: 910, total steps: 253506, score: -21.0\n",
      "episode: 277/100000, steps: 850, total steps: 254356, score: -21.0\n",
      "episode: 278/100000, steps: 965, total steps: 255321, score: -19.0\n",
      "episode: 279/100000, steps: 919, total steps: 256240, score: -20.0\n",
      "episode: 280/100000, steps: 938, total steps: 257178, score: -19.0\n",
      "episode: 281/100000, steps: 823, total steps: 258001, score: -21.0\n",
      "episode: 282/100000, steps: 869, total steps: 258870, score: -20.0\n",
      "episode: 283/100000, steps: 851, total steps: 259721, score: -21.0\n",
      "episode: 284/100000, steps: 1002, total steps: 260723, score: -19.0\n",
      "episode: 285/100000, steps: 869, total steps: 261592, score: -21.0\n",
      "episode: 286/100000, steps: 989, total steps: 262581, score: -20.0\n",
      "episode: 287/100000, steps: 987, total steps: 263568, score: -21.0\n",
      "episode: 288/100000, steps: 1005, total steps: 264573, score: -19.0\n",
      "episode: 289/100000, steps: 923, total steps: 265496, score: -20.0\n",
      "episode: 290/100000, steps: 869, total steps: 266365, score: -20.0\n",
      "episode: 291/100000, steps: 920, total steps: 267285, score: -21.0\n",
      "episode: 292/100000, steps: 989, total steps: 268274, score: -21.0\n",
      "episode: 293/100000, steps: 962, total steps: 269236, score: -20.0\n",
      "Saving model: atariPong270000.h5\n",
      "episode: 294/100000, steps: 809, total steps: 270045, score: -21.0\n",
      "episode: 295/100000, steps: 823, total steps: 270868, score: -21.0\n",
      "episode: 296/100000, steps: 1017, total steps: 271885, score: -20.0\n",
      "episode: 297/100000, steps: 874, total steps: 272759, score: -21.0\n",
      "episode: 298/100000, steps: 762, total steps: 273521, score: -21.0\n",
      "episode: 299/100000, steps: 781, total steps: 274302, score: -21.0\n",
      "episode: 300/100000, steps: 897, total steps: 275199, score: -20.0\n",
      "episode: 301/100000, steps: 882, total steps: 276081, score: -21.0\n",
      "episode: 302/100000, steps: 762, total steps: 276843, score: -21.0\n",
      "episode: 303/100000, steps: 1014, total steps: 277857, score: -21.0\n",
      "episode: 304/100000, steps: 953, total steps: 278810, score: -19.0\n",
      "episode: 305/100000, steps: 939, total steps: 279749, score: -21.0\n",
      "episode: 306/100000, steps: 926, total steps: 280675, score: -20.0\n",
      "episode: 307/100000, steps: 762, total steps: 281437, score: -21.0\n",
      "episode: 308/100000, steps: 992, total steps: 282429, score: -19.0\n",
      "episode: 309/100000, steps: 931, total steps: 283360, score: -21.0\n",
      "episode: 310/100000, steps: 942, total steps: 284302, score: -21.0\n",
      "episode: 311/100000, steps: 871, total steps: 285173, score: -21.0\n",
      "episode: 312/100000, steps: 1002, total steps: 286175, score: -19.0\n",
      "episode: 313/100000, steps: 1000, total steps: 287175, score: -21.0\n",
      "episode: 314/100000, steps: 846, total steps: 288021, score: -21.0\n",
      "episode: 315/100000, steps: 976, total steps: 288997, score: -20.0\n",
      "episode: 316/100000, steps: 840, total steps: 289837, score: -20.0\n",
      "episode: 317/100000, steps: 822, total steps: 290659, score: -21.0\n",
      "episode: 318/100000, steps: 957, total steps: 291616, score: -20.0\n",
      "episode: 319/100000, steps: 762, total steps: 292378, score: -21.0\n",
      "episode: 320/100000, steps: 900, total steps: 293278, score: -20.0\n",
      "episode: 321/100000, steps: 841, total steps: 294119, score: -20.0\n",
      "episode: 322/100000, steps: 1035, total steps: 295154, score: -20.0\n",
      "episode: 323/100000, steps: 993, total steps: 296147, score: -21.0\n",
      "episode: 324/100000, steps: 841, total steps: 296988, score: -21.0\n",
      "episode: 325/100000, steps: 790, total steps: 297778, score: -21.0\n",
      "episode: 326/100000, steps: 988, total steps: 298766, score: -20.0\n",
      "episode: 327/100000, steps: 1034, total steps: 299800, score: -21.0\n",
      "Saving model: atariPong300000.h5\n",
      "episode: 328/100000, steps: 871, total steps: 300671, score: -21.0\n",
      "episode: 329/100000, steps: 880, total steps: 301551, score: -21.0\n",
      "episode: 330/100000, steps: 942, total steps: 302493, score: -21.0\n",
      "episode: 331/100000, steps: 884, total steps: 303377, score: -21.0\n",
      "episode: 332/100000, steps: 868, total steps: 304245, score: -20.0\n",
      "episode: 333/100000, steps: 822, total steps: 305067, score: -21.0\n",
      "episode: 334/100000, steps: 982, total steps: 306049, score: -20.0\n",
      "episode: 335/100000, steps: 901, total steps: 306950, score: -21.0\n",
      "episode: 336/100000, steps: 1021, total steps: 307971, score: -20.0\n",
      "episode: 337/100000, steps: 900, total steps: 308871, score: -20.0\n",
      "episode: 338/100000, steps: 840, total steps: 309711, score: -20.0\n",
      "episode: 339/100000, steps: 850, total steps: 310561, score: -21.0\n",
      "episode: 340/100000, steps: 928, total steps: 311489, score: -20.0\n",
      "episode: 341/100000, steps: 940, total steps: 312429, score: -21.0\n",
      "episode: 342/100000, steps: 962, total steps: 313391, score: -20.0\n",
      "episode: 343/100000, steps: 852, total steps: 314243, score: -21.0\n",
      "episode: 344/100000, steps: 822, total steps: 315065, score: -21.0\n",
      "episode: 345/100000, steps: 1105, total steps: 316170, score: -20.0\n",
      "episode: 346/100000, steps: 975, total steps: 317145, score: -20.0\n",
      "episode: 347/100000, steps: 975, total steps: 318120, score: -19.0\n",
      "episode: 348/100000, steps: 953, total steps: 319073, score: -20.0\n",
      "episode: 349/100000, steps: 790, total steps: 319863, score: -21.0\n",
      "episode: 350/100000, steps: 963, total steps: 320826, score: -21.0\n",
      "episode: 351/100000, steps: 1105, total steps: 321931, score: -19.0\n",
      "episode: 352/100000, steps: 850, total steps: 322781, score: -21.0\n",
      "episode: 353/100000, steps: 843, total steps: 323624, score: -21.0\n",
      "episode: 354/100000, steps: 762, total steps: 324386, score: -21.0\n",
      "episode: 355/100000, steps: 850, total steps: 325236, score: -21.0\n",
      "episode: 356/100000, steps: 955, total steps: 326191, score: -19.0\n",
      "episode: 357/100000, steps: 1042, total steps: 327233, score: -18.0\n",
      "episode: 358/100000, steps: 917, total steps: 328150, score: -20.0\n",
      "episode: 359/100000, steps: 762, total steps: 328912, score: -21.0\n",
      "episode: 360/100000, steps: 883, total steps: 329795, score: -21.0\n",
      "Saving model: atariPong330000.h5\n",
      "episode: 361/100000, steps: 840, total steps: 330635, score: -20.0\n",
      "episode: 362/100000, steps: 942, total steps: 331577, score: -21.0\n",
      "episode: 363/100000, steps: 1008, total steps: 332585, score: -20.0\n",
      "episode: 364/100000, steps: 762, total steps: 333347, score: -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 365/100000, steps: 822, total steps: 334169, score: -21.0\n",
      "episode: 366/100000, steps: 1043, total steps: 335212, score: -19.0\n",
      "episode: 367/100000, steps: 889, total steps: 336101, score: -20.0\n",
      "episode: 368/100000, steps: 972, total steps: 337073, score: -20.0\n",
      "episode: 369/100000, steps: 1043, total steps: 338116, score: -19.0\n",
      "episode: 370/100000, steps: 841, total steps: 338957, score: -21.0\n",
      "episode: 371/100000, steps: 947, total steps: 339904, score: -21.0\n",
      "episode: 372/100000, steps: 843, total steps: 340747, score: -21.0\n",
      "episode: 373/100000, steps: 842, total steps: 341589, score: -21.0\n",
      "episode: 374/100000, steps: 1198, total steps: 342787, score: -18.0\n",
      "episode: 375/100000, steps: 1149, total steps: 343936, score: -19.0\n",
      "episode: 376/100000, steps: 841, total steps: 344777, score: -20.0\n",
      "episode: 377/100000, steps: 1010, total steps: 345787, score: -18.0\n",
      "episode: 378/100000, steps: 840, total steps: 346627, score: -20.0\n",
      "episode: 379/100000, steps: 818, total steps: 347445, score: -21.0\n",
      "episode: 380/100000, steps: 841, total steps: 348286, score: -21.0\n",
      "episode: 381/100000, steps: 850, total steps: 349136, score: -21.0\n",
      "episode: 382/100000, steps: 762, total steps: 349898, score: -21.0\n",
      "episode: 383/100000, steps: 885, total steps: 350783, score: -20.0\n",
      "episode: 384/100000, steps: 762, total steps: 351545, score: -21.0\n",
      "episode: 385/100000, steps: 1110, total steps: 352655, score: -18.0\n",
      "episode: 386/100000, steps: 1021, total steps: 353676, score: -20.0\n",
      "episode: 387/100000, steps: 880, total steps: 354556, score: -21.0\n",
      "episode: 388/100000, steps: 963, total steps: 355519, score: -19.0\n",
      "episode: 389/100000, steps: 850, total steps: 356369, score: -21.0\n",
      "episode: 390/100000, steps: 863, total steps: 357232, score: -20.0\n",
      "episode: 391/100000, steps: 762, total steps: 357994, score: -21.0\n",
      "episode: 392/100000, steps: 792, total steps: 358786, score: -21.0\n",
      "episode: 393/100000, steps: 846, total steps: 359632, score: -21.0\n",
      "Saving model: atariPong360000.h5\n",
      "episode: 394/100000, steps: 884, total steps: 360516, score: -21.0\n",
      "episode: 395/100000, steps: 949, total steps: 361465, score: -20.0\n",
      "episode: 396/100000, steps: 781, total steps: 362246, score: -21.0\n",
      "episode: 397/100000, steps: 1055, total steps: 363301, score: -19.0\n",
      "episode: 398/100000, steps: 880, total steps: 364181, score: -21.0\n",
      "episode: 399/100000, steps: 896, total steps: 365077, score: -20.0\n",
      "episode: 400/100000, steps: 841, total steps: 365918, score: -20.0\n",
      "episode: 401/100000, steps: 953, total steps: 366871, score: -20.0\n",
      "episode: 402/100000, steps: 900, total steps: 367771, score: -20.0\n",
      "episode: 403/100000, steps: 915, total steps: 368686, score: -21.0\n",
      "episode: 404/100000, steps: 781, total steps: 369467, score: -21.0\n",
      "episode: 405/100000, steps: 792, total steps: 370259, score: -21.0\n",
      "episode: 406/100000, steps: 908, total steps: 371167, score: -21.0\n",
      "episode: 407/100000, steps: 822, total steps: 371989, score: -21.0\n",
      "episode: 408/100000, steps: 1266, total steps: 373255, score: -19.0\n",
      "episode: 409/100000, steps: 1000, total steps: 374255, score: -21.0\n",
      "episode: 410/100000, steps: 870, total steps: 375125, score: -21.0\n",
      "episode: 411/100000, steps: 828, total steps: 375953, score: -21.0\n",
      "episode: 412/100000, steps: 822, total steps: 376775, score: -21.0\n",
      "episode: 413/100000, steps: 863, total steps: 377638, score: -20.0\n",
      "episode: 414/100000, steps: 824, total steps: 378462, score: -21.0\n",
      "episode: 415/100000, steps: 1061, total steps: 379523, score: -21.0\n",
      "episode: 416/100000, steps: 841, total steps: 380364, score: -21.0\n",
      "episode: 417/100000, steps: 850, total steps: 381214, score: -21.0\n",
      "episode: 418/100000, steps: 824, total steps: 382038, score: -21.0\n",
      "episode: 419/100000, steps: 781, total steps: 382819, score: -21.0\n",
      "episode: 420/100000, steps: 790, total steps: 383609, score: -21.0\n",
      "episode: 421/100000, steps: 850, total steps: 384459, score: -21.0\n",
      "episode: 422/100000, steps: 955, total steps: 385414, score: -19.0\n",
      "episode: 423/100000, steps: 762, total steps: 386176, score: -21.0\n",
      "episode: 424/100000, steps: 762, total steps: 386938, score: -21.0\n",
      "episode: 425/100000, steps: 822, total steps: 387760, score: -21.0\n",
      "episode: 426/100000, steps: 762, total steps: 388522, score: -21.0\n",
      "episode: 427/100000, steps: 1222, total steps: 389744, score: -19.0\n",
      "Saving model: atariPong390000.h5\n",
      "episode: 428/100000, steps: 993, total steps: 390737, score: -19.0\n",
      "episode: 429/100000, steps: 896, total steps: 391633, score: -20.0\n",
      "episode: 430/100000, steps: 842, total steps: 392475, score: -21.0\n",
      "episode: 431/100000, steps: 822, total steps: 393297, score: -21.0\n",
      "episode: 432/100000, steps: 762, total steps: 394059, score: -21.0\n",
      "episode: 433/100000, steps: 850, total steps: 394909, score: -21.0\n",
      "episode: 434/100000, steps: 809, total steps: 395718, score: -21.0\n",
      "episode: 435/100000, steps: 921, total steps: 396639, score: -21.0\n",
      "episode: 436/100000, steps: 935, total steps: 397574, score: -19.0\n",
      "episode: 437/100000, steps: 835, total steps: 398409, score: -20.0\n",
      "episode: 438/100000, steps: 840, total steps: 399249, score: -20.0\n",
      "episode: 439/100000, steps: 762, total steps: 400011, score: -21.0\n",
      "episode: 440/100000, steps: 921, total steps: 400932, score: -20.0\n",
      "episode: 441/100000, steps: 824, total steps: 401756, score: -21.0\n",
      "episode: 442/100000, steps: 1007, total steps: 402763, score: -20.0\n",
      "episode: 443/100000, steps: 810, total steps: 403573, score: -21.0\n",
      "episode: 444/100000, steps: 762, total steps: 404335, score: -21.0\n",
      "episode: 445/100000, steps: 985, total steps: 405320, score: -20.0\n",
      "episode: 446/100000, steps: 850, total steps: 406170, score: -21.0\n",
      "episode: 447/100000, steps: 935, total steps: 407105, score: -19.0\n",
      "episode: 448/100000, steps: 970, total steps: 408075, score: -19.0\n",
      "episode: 449/100000, steps: 850, total steps: 408925, score: -21.0\n",
      "episode: 450/100000, steps: 762, total steps: 409687, score: -21.0\n",
      "episode: 451/100000, steps: 910, total steps: 410597, score: -21.0\n",
      "episode: 452/100000, steps: 882, total steps: 411479, score: -21.0\n",
      "episode: 453/100000, steps: 762, total steps: 412241, score: -21.0\n",
      "episode: 454/100000, steps: 883, total steps: 413124, score: -21.0\n",
      "episode: 455/100000, steps: 790, total steps: 413914, score: -21.0\n",
      "episode: 456/100000, steps: 792, total steps: 414706, score: -21.0\n",
      "episode: 457/100000, steps: 902, total steps: 415608, score: -20.0\n",
      "episode: 458/100000, steps: 945, total steps: 416553, score: -20.0\n",
      "episode: 459/100000, steps: 939, total steps: 417492, score: -21.0\n",
      "episode: 460/100000, steps: 822, total steps: 418314, score: -21.0\n",
      "episode: 461/100000, steps: 878, total steps: 419192, score: -21.0\n",
      "Saving model: atariPong420000.h5\n",
      "episode: 462/100000, steps: 884, total steps: 420076, score: -21.0\n",
      "episode: 463/100000, steps: 882, total steps: 420958, score: -21.0\n",
      "episode: 464/100000, steps: 919, total steps: 421877, score: -20.0\n",
      "episode: 465/100000, steps: 924, total steps: 422801, score: -20.0\n",
      "episode: 466/100000, steps: 762, total steps: 423563, score: -21.0\n",
      "episode: 467/100000, steps: 838, total steps: 424401, score: -21.0\n",
      "episode: 468/100000, steps: 840, total steps: 425241, score: -20.0\n",
      "episode: 469/100000, steps: 762, total steps: 426003, score: -21.0\n",
      "episode: 470/100000, steps: 822, total steps: 426825, score: -21.0\n",
      "episode: 471/100000, steps: 790, total steps: 427615, score: -21.0\n",
      "episode: 472/100000, steps: 887, total steps: 428502, score: -20.0\n",
      "episode: 473/100000, steps: 841, total steps: 429343, score: -20.0\n",
      "episode: 474/100000, steps: 870, total steps: 430213, score: -21.0\n",
      "episode: 475/100000, steps: 852, total steps: 431065, score: -21.0\n",
      "episode: 476/100000, steps: 958, total steps: 432023, score: -20.0\n",
      "episode: 477/100000, steps: 846, total steps: 432869, score: -21.0\n",
      "episode: 478/100000, steps: 822, total steps: 433691, score: -21.0\n",
      "episode: 479/100000, steps: 762, total steps: 434453, score: -21.0\n",
      "episode: 480/100000, steps: 925, total steps: 435378, score: -21.0\n",
      "episode: 481/100000, steps: 912, total steps: 436290, score: -21.0\n",
      "episode: 482/100000, steps: 841, total steps: 437131, score: -21.0\n",
      "episode: 483/100000, steps: 900, total steps: 438031, score: -20.0\n",
      "episode: 484/100000, steps: 762, total steps: 438793, score: -21.0\n",
      "episode: 485/100000, steps: 910, total steps: 439703, score: -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 486/100000, steps: 790, total steps: 440493, score: -21.0\n",
      "episode: 487/100000, steps: 878, total steps: 441371, score: -21.0\n",
      "episode: 488/100000, steps: 824, total steps: 442195, score: -21.0\n",
      "episode: 489/100000, steps: 882, total steps: 443077, score: -21.0\n",
      "episode: 490/100000, steps: 824, total steps: 443901, score: -21.0\n",
      "episode: 491/100000, steps: 762, total steps: 444663, score: -21.0\n",
      "episode: 492/100000, steps: 850, total steps: 445513, score: -21.0\n",
      "episode: 493/100000, steps: 840, total steps: 446353, score: -20.0\n",
      "episode: 494/100000, steps: 762, total steps: 447115, score: -21.0\n",
      "episode: 495/100000, steps: 822, total steps: 447937, score: -21.0\n",
      "episode: 496/100000, steps: 921, total steps: 448858, score: -20.0\n",
      "episode: 497/100000, steps: 1025, total steps: 449883, score: -19.0\n",
      "Saving model: atariPong450000.h5\n",
      "episode: 498/100000, steps: 762, total steps: 450645, score: -21.0\n",
      "episode: 499/100000, steps: 840, total steps: 451485, score: -21.0\n",
      "episode: 500/100000, steps: 790, total steps: 452275, score: -21.0\n",
      "episode: 501/100000, steps: 902, total steps: 453177, score: -20.0\n",
      "episode: 502/100000, steps: 822, total steps: 453999, score: -21.0\n",
      "episode: 503/100000, steps: 822, total steps: 454821, score: -21.0\n",
      "episode: 504/100000, steps: 870, total steps: 455691, score: -20.0\n",
      "episode: 505/100000, steps: 945, total steps: 456636, score: -20.0\n",
      "episode: 506/100000, steps: 762, total steps: 457398, score: -21.0\n",
      "episode: 507/100000, steps: 877, total steps: 458275, score: -20.0\n",
      "episode: 508/100000, steps: 887, total steps: 459162, score: -20.0\n",
      "episode: 509/100000, steps: 762, total steps: 459924, score: -21.0\n",
      "episode: 510/100000, steps: 971, total steps: 460895, score: -21.0\n",
      "episode: 511/100000, steps: 875, total steps: 461770, score: -20.0\n",
      "episode: 512/100000, steps: 762, total steps: 462532, score: -21.0\n",
      "episode: 513/100000, steps: 781, total steps: 463313, score: -21.0\n",
      "episode: 514/100000, steps: 822, total steps: 464135, score: -21.0\n",
      "episode: 515/100000, steps: 972, total steps: 465107, score: -19.0\n",
      "episode: 516/100000, steps: 781, total steps: 465888, score: -21.0\n",
      "episode: 517/100000, steps: 865, total steps: 466753, score: -20.0\n",
      "episode: 518/100000, steps: 822, total steps: 467575, score: -21.0\n",
      "episode: 519/100000, steps: 944, total steps: 468519, score: -21.0\n",
      "episode: 520/100000, steps: 762, total steps: 469281, score: -21.0\n",
      "episode: 521/100000, steps: 790, total steps: 470071, score: -21.0\n",
      "episode: 522/100000, steps: 869, total steps: 470940, score: -21.0\n",
      "episode: 523/100000, steps: 917, total steps: 471857, score: -20.0\n",
      "episode: 524/100000, steps: 1064, total steps: 472921, score: -21.0\n",
      "episode: 525/100000, steps: 1019, total steps: 473940, score: -21.0\n",
      "episode: 526/100000, steps: 911, total steps: 474851, score: -21.0\n",
      "episode: 527/100000, steps: 809, total steps: 475660, score: -21.0\n",
      "episode: 528/100000, steps: 945, total steps: 476605, score: -20.0\n",
      "episode: 529/100000, steps: 840, total steps: 477445, score: -20.0\n",
      "episode: 530/100000, steps: 762, total steps: 478207, score: -21.0\n",
      "episode: 531/100000, steps: 792, total steps: 478999, score: -21.0\n",
      "episode: 532/100000, steps: 852, total steps: 479851, score: -21.0\n",
      "Saving model: atariPong480000.h5\n",
      "episode: 533/100000, steps: 762, total steps: 480613, score: -21.0\n",
      "episode: 534/100000, steps: 762, total steps: 481375, score: -21.0\n",
      "episode: 535/100000, steps: 990, total steps: 482365, score: -21.0\n",
      "episode: 536/100000, steps: 853, total steps: 483218, score: -21.0\n",
      "episode: 537/100000, steps: 919, total steps: 484137, score: -20.0\n",
      "episode: 538/100000, steps: 913, total steps: 485050, score: -20.0\n",
      "episode: 539/100000, steps: 880, total steps: 485930, score: -21.0\n",
      "episode: 540/100000, steps: 790, total steps: 486720, score: -21.0\n",
      "episode: 541/100000, steps: 818, total steps: 487538, score: -21.0\n",
      "episode: 542/100000, steps: 822, total steps: 488360, score: -21.0\n",
      "episode: 543/100000, steps: 871, total steps: 489231, score: -21.0\n",
      "episode: 544/100000, steps: 970, total steps: 490201, score: -21.0\n",
      "episode: 545/100000, steps: 762, total steps: 490963, score: -21.0\n",
      "episode: 546/100000, steps: 896, total steps: 491859, score: -20.0\n",
      "episode: 547/100000, steps: 762, total steps: 492621, score: -21.0\n",
      "episode: 548/100000, steps: 822, total steps: 493443, score: -21.0\n",
      "episode: 549/100000, steps: 841, total steps: 494284, score: -20.0\n",
      "episode: 550/100000, steps: 792, total steps: 495076, score: -21.0\n",
      "episode: 551/100000, steps: 878, total steps: 495954, score: -21.0\n",
      "episode: 552/100000, steps: 852, total steps: 496806, score: -21.0\n",
      "episode: 553/100000, steps: 762, total steps: 497568, score: -21.0\n",
      "episode: 554/100000, steps: 952, total steps: 498520, score: -20.0\n",
      "episode: 555/100000, steps: 910, total steps: 499430, score: -21.0\n",
      "episode: 556/100000, steps: 762, total steps: 500192, score: -21.0\n",
      "episode: 557/100000, steps: 840, total steps: 501032, score: -20.0\n",
      "episode: 558/100000, steps: 908, total steps: 501940, score: -21.0\n",
      "episode: 559/100000, steps: 852, total steps: 502792, score: -21.0\n",
      "episode: 560/100000, steps: 781, total steps: 503573, score: -21.0\n",
      "episode: 561/100000, steps: 868, total steps: 504441, score: -20.0\n",
      "episode: 562/100000, steps: 762, total steps: 505203, score: -21.0\n",
      "episode: 563/100000, steps: 850, total steps: 506053, score: -21.0\n",
      "episode: 564/100000, steps: 1076, total steps: 507129, score: -20.0\n",
      "episode: 565/100000, steps: 823, total steps: 507952, score: -21.0\n"
     ]
    }
   ],
   "source": [
    "# initialize gym environment and the agent\n",
    "game = 'Pong'\n",
    "env = wrap_deepmind(gym.make(game + 'Deterministic-v4'), frame_stack=True)\n",
    "observation_space = env.observation_space.shape\n",
    "action_space = env.action_space.n\n",
    "print('Observation space: ', observation_space)\n",
    "print('Action space: ', action_space)\n",
    "agent = DDQNAgent(game, observation_space, action_space)\n",
    "episodes = 100000\n",
    "total_step = 0\n",
    "# Iterate the game\n",
    "for e in range(episodes):\n",
    "    # reset state in the beginning of each game\n",
    "    state = env.reset()\n",
    "    # time_t represents each frame of the game\n",
    "    # Our goal is to keep the pole upright as long as possible until score of 500\n",
    "    # the more time_t the more score\n",
    "    step = 0\n",
    "    score = 0\n",
    "    while True:\n",
    "        total_step += 1\n",
    "        step += 1\n",
    "        # turn this on if you want to render\n",
    "        #env.render()\n",
    "        # Decide action\n",
    "        action = agent.act(state)\n",
    "        # Advance the game to the next frame based on the action.\n",
    "        # Reward is 1 for every frame the pole survived\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        # Remember the previous state, action, reward, and done\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "        # train the agent with the experience of the episode\n",
    "        agent.replay(total_step, 32)\n",
    "        # done becomes True when the game ends\n",
    "        # ex) The agent drops the pole\n",
    "        if done:\n",
    "            # print the score and break out of the loop\n",
    "            print(\"episode: {}/{}, steps: {}, total steps: {}, score: {}\"\n",
    "                  .format(e, episodes, step, total_step, score))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'env2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5830fbf6662e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrap_deepmind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PongDeterministic-v4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_stack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0menv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env2' is not defined"
     ]
    }
   ],
   "source": [
    "from gym.utils.play import play\n",
    "%matplotlib inline\n",
    "play(wrap_deepmind(gym.make('PongDeterministic-v4'), frame_stack=True, scale=True))\n",
    "state = env.reset()\n",
    "print(env2.observation_space.shape)\n",
    "plt.imshow(state)\n",
    "env2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
